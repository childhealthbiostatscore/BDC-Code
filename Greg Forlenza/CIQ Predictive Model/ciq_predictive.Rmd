---
title: "Control IQ Predictive Model"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
library(Hmisc)
library(arsenal)
library(tidyverse)
library(lubridate)
library(knitr)
library(DT)
library(naniar)
library(glmnet)
library(broom)
library(pROC)
knitr::opts_chunk$set(echo = FALSE)
home_dir = ifelse(.Platform$OS.type != "unix",
                  "Z:/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Greg Forlenza/Control IQ Prediction",
                  "~/UCD/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Greg Forlenza/Control IQ Prediction")
knitr::opts_knit$set(root.dir = home_dir)
```

```{r import}
# Import
source("C:/Users/timvigers/Documents/GitHub/BDC-Code/Greg Forlenza/CIQ Predictive Model/clean_data.R")
# Add new variables
data = data %>% group_by(record_id) %>% 
  # Fill down demographics
  fill(demographics_dob,demographics_sex.factor,demographics_diabetesdx,
       demographics_race.factor,demographics_ethnicity.factor,
       demographics_cgmhx.factor,demographics_pumphx.factor,
       demographics_insurance.factor) %>% 
  ungroup() %>%
  # Calculate age and T1D duration
  mutate(age_visit = round(as.numeric(ymd(date_visit)-ymd(demographics_dob),
                                      units="days")/365.25),
         t1d_dur = round(as.numeric(ymd(date_visit)-ymd(demographics_diabetesdx),
                                    units="days")/365.25)) %>%
  # Combine columns
  unite(age,age_visit,demographics_age,na.rm = T) %>%
  unite(t1d_dur,demographics_t1d_duration,t1d_dur,na.rm = T) %>%
  unite(race,demographics_race.factor,demographics_ethnicity.factor) %>%
  # Fix names
  rename(sex = "demographics_sex.factor",insurance = "demographics_insurance.factor",
         cgm_hx = "demographics_cgmhx.factor",pump_hx = "demographics_pumphx.factor") 
# Clean up variables
data$age = as.numeric(sapply(strsplit(data$age,"_"),"[",1))
data$t1d_dur = as.numeric(sapply(strsplit(data$t1d_dur,"_"),"[",1))
data$race = factor(data$race)
levels(data$race) = c("Non-White","Non-White","Non-White","Non-White","Non-White",
                      "Non-White","Non-White","Unknown","Non-White","Non-White",
                      "Non-White","Unknown","Unknown","Non-White",
                      "Non-Hispanic White","Non-White")
# Survey scoring
# INSPIRE
data$inspire_child = 
  apply(data[,grep("inspire_b\\d{,2}$",colnames(data))],1,function(r){
    # 5 is a missing value
    r[r==5] = NA
    mean(r,na.rm = T)*25
  })
data$inspire_child[is.nan(data$inspire_child)] = NA
data$inspire_adult = 
  apply(data[,grep("inspire_b\\d{,2}_adult$",colnames(data))],1,function(r){
    r[r==5] = NA
    mean(r,na.rm = T)*25
  })
data$inspire_adult[is.nan(data$inspire_adult)] = NA
data = data %>% unite(inspire,inspire_child,inspire_adult,na.rm = T)
data$inspire = as.numeric(data$inspire)
# PAID
data$paid_child = 
  apply(data[,grep("c_paid\\d{,2}$",colnames(data))],1,function(r){
    r = 5 - r # Reverse score
    mean(r,na.rm = T)*25
  })
data$paid_child[is.nan(data$paid_child)] = NA
data$paid_ya = 
  apply(data[,grep("ya_paid\\d{,2}$",colnames(data))],1,function(r){
    sum(r)*1.25
  })
data = data %>% unite(paid,paid_child,paid_ya,na.rm = T)
data$paid = as.numeric(data$paid)
# HFS worry
worry_low_bg = c(11,12,13,14,16,18,19,22,23)
data$hfs_worry_low_bg = apply(data[,paste0("c_hfs_worry",worry_low_bg)],1,function(r){
  sum(r)
})
worry_social = c(15,17,20,21,25)
data$hfs_worry_social = apply(data[,paste0("c_hfs_worry",worry_social)],1,function(r){
  sum(r)
})
# Month 1 variables
m1 = data %>% filter(gyl_timepoint == 1) %>% 
  select(record_id,sensor_wear,sensor_70_180,dailybolus,dailymealbolus,time_am)
colnames(m1)[2:ncol(m1)] = paste0("m1_",colnames(m1)[2:ncol(m1)])
# Outcome
out = data %>% group_by(record_id) %>% 
  filter(gyl_timepoint.factor =="12 Months" | 
           gyl_timepoint.factor =="9 Months") %>%
  filter(row_number() == n()) %>%
  summarise(success = sensor_70_180)
out$success = cut(out$success,c(-Inf,70,Inf),labels = c("<70",">=70"),
                  right = F)
# Final data
df = data %>% left_join(.,m1,by = "record_id") %>% left_join(.,out,by = "record_id") %>%
  filter(record_id %in% 119:327,!is.na(gyl_timepoint.factor),
         gyl_timepoint == 0) %>%
  select(record_id,hba1c,age,t1d_dur,sex,race,insurance,
         cgm_hx,pump_hx,starts_with("m1_"),inspire,paid,
         hfs_worry_low_bg,hfs_worry_social,success) %>%
  filter(!is.na(success))
# Clean up workspace
rm(list = ls()[-which(ls() == "df")])
# Write cleaned data
write.csv(df,file = "./Data_Cleaned/ciq_cleaned.csv",row.names = F)
```

# Table 1: Participant Characteristics

Race and ethnicity were combined into a single field (non-Hispanic White vs. non-White). Success was defined as TIR $\geq70%$ after 12 months (or 9 months if missing 12 month data).

```{r results='asis'}
t1_form = as.formula(paste0("success~",paste0(colnames(df)[2:ncol(df)],collapse = "+")))
t1_form = update(t1_form,.~. -success)
t1 = tableby(t1_form,df)
summary(t1)
```

# Missing data

A large proportion of the participants were missing data, so only 45 participants were included in the elasticnet (a variation of the lasso) model selection process.

```{r}
vis_miss(df)
gg_miss_upset(df)
```

# Elasticnet variable selection

```{r}
set.seed(1017)
lasso_df = df[complete.cases(df),]
outcome = "success"
predictors = colnames(lasso_df)[which(!colnames(lasso_df) %in% c("record_id","success"))]
cv = cv.glmnet(x = data.matrix(lasso_df[,predictors]),
               y = data.matrix(lasso_df[,outcome]),alpha = 0.5)
```

Elasticnet (a variation of the lasso) parameter $\lambda$ was selected based on 5-fold cross validation. There are two recommended ways to looks at lasso/elasticnet selection: picking the model with the lowest CV error, or the smallest model with CV error within 1 standard deviation of the minimum.

Variables included in the selection process were: `r paste(predictors,collapse=", ")`.

## Smallest model 

The smallest possible model with reasonable CV error includes:

```{r}
selected = coef(cv,s = "lambda.1se")
selected = rownames(selected)[which(selected != 0 & rownames(selected) != "(Intercept)")]
kable(selected,col.names = "Selected Variable(s)")
```

### Plot and Coefficients

```{r}
form = as.formula(paste0("success~",paste0(selected,collapse = "+")))
mod = glm(form,data = df,family = "binomial")
r = roc(mod$y,predict(mod),quiet = T)
plot.roc(r,plot = T,legacy.axes = T,print.auc = T)
kable(tidy(mod,exponentiate = T),digits = 3)
```

This model was fit using `r nobs(mod)` observations.

## Best model 

The model with the lowest CV error includes:

```{r}
selected = coef(cv,s = "lambda.min")
selected = rownames(selected)[which(selected != 0 & rownames(selected) != "(Intercept)")]
kable(selected,col.names = "Selected Variable(s)")
```

### ROC and Coefficients

```{r}
form = as.formula(paste0("success~",paste0(selected,collapse = "+")))
mod = glm(form,data = df,family = "binomial")
r = roc(mod$y,predict(mod),quiet = T)
plot.roc(r,plot = T,legacy.axes = T,print.auc = T)
kable(tidy(mod,exponentiate = T),digits = 3)
```

This model was fit using `r nobs(mod)` observations.
