---
title: "Daily Predictors for Diabetes Management"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
library(Hmisc)
library(arsenal)
library(skimr)
library(knitr)
library(tidyverse)
library(lubridate)
library(redcapAPI)
library(glmmLasso)
library(parallel)
knitr::opts_chunk$set(echo = F)
home_dir = ifelse(.Platform$OS.type != "unix","T:/",
                  "/mnt/UCD/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects")
knitr::opts_knit$set(root.dir = home_dir)
set.seed(1017)
```

```{r clean data}
source("~/Documents/GitHub/BDC-Code/Laurel Messer/Daily Predictors/clean_data.r")
# Combine race columns
race_cols = paste0("pt_race___",1:6)
races = c("White","Black or African American","American Indian or Alaskan Native",
          "Asian","Native Hawaiian or Pacific Islander","Other")
data$race = apply(data[,race_cols], 1, function(r){
  paste0(races[which(r == 1)],collapse = ", ")
})
# Calculate age and duration
data$age = as.numeric(ymd(data$pt_visit_date) - ymd(data$pt_dob))/365.25
data$duration = as.numeric(ymd(data$pt_visit_date) - ymd(data$dxdate))/365.25
```

# Participant Characteristics

```{r results = 'asis'}
# Get first row for each person
demographics = data %>% group_by(record_id) %>% filter(row_number() == 1)
# Table 
t1 = tableby(~ age + duration + pt_a1c + pt_baseline1.factor + pt_baseline5.factor + 
               pt_gender.factor + race + pt_eth.factor + p1_hedu.factor + 
               p2_hedu.factor,demographics)
# Labels
new_labels = list(age = "Age", duration = "Diabetes Duration", pt_a1c = "Baseline HbA1c",
                  pt_baseline1.factor = "Pump Hx",pt_baseline5.factor = "CGM Hx",
                  pt_gender.factor = "Gender", race = "Race", pt_eth.factor = "Ethnicity", 
                  p1_hedu.factor = "Parent 1 Education", p2_hedu.factor = "Parent 2 Education")
summary(t1,labelTranslations = new_labels)
```

# Regularization (LASSO) Results

```{r}
# Correlation between engagement survey items and subscores and:
#   Number of insulin boluses
#   Average glucose level
#   Percent time glucose levels 70-180 mg/dl (“Time-in-Range”, TIR)
#   Goal Survey Scores
morning_items = colnames(data)[grep("esq\\d+$",colnames(data))]
evening_items = colnames(data)[grep("esq\\d+_eve$",colnames(data))]
# Outcomes
goal_survey = c("n_highalerts","dm_caretime","dm_thinktime","gsq1","gsq2","gsq3")
# ID Variables
id_vars = c("record_id","redcap_event_name","es_datetimecapture")
# Optimal lambda function = needs id variables, one outcome, and predictors (nothing else)
find_lambda = function(df,outcome,metric = "bic",lambdas = seq(0,1000),cores = 8){
  # IDs must be factor
  df$record_id = as.factor(df$record_id)
  # Make model formula
  pred = colnames(df)[which(!colnames(df) %in% c(outcome,id_vars))]
  form = as.formula(paste(outcome,"~",paste(pred,collapse = "+")))
  # Try all the different lambdas
  cl = makeCluster(cores,type = "FORK")
  metrics = parLapply(cl,lambdas, function(l){
    mod <- try(glmmLasso(form, rnd = list(record_id=~1),data = df,lambda=l,
                         control = list(method = "REML")))
    ifelse(class(mod)!="try-error",mod[[metric]],NA)})
  stopCluster(cl)
  metrics = unlist(metrics)
  # Get the best lambda
  best_lambda = lambdas[which.min(metrics)]
  return(best_lambda)
}
```

## Number of boluses

### Morning survey

```{r}
bolus_morning = data[,c(id_vars,morning_items,"boluses")]
bolus_morning = bolus_morning[complete.cases(bolus_morning),]
lambda = find_lambda(df = bolus_morning,outcome = "boluses")
```

### Evening survey

```{r}
# Example
data("soccer")
## generalized additive mixed model
## grid for the smoothing parameter

## center all metric variables so that also the starting values with glmmPQL are in the correct scaling

soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=T,scale=T)
soccer<-data.frame(soccer)


lambda <- seq(500,0,by=-5)

family = poisson(link = log)


################## First Simple Method ############################################
## Using BIC (or AIC, respectively) to determine the optimal tuning parameter lambda

BIC_vec<-rep(Inf,length(lambda))

## first fit good starting model
library(MASS);library(nlme)
PQL<-glmmPQL(points~1,random = ~1|team,family=family,data=soccer)
Delta.start<-c(as.numeric(PQL$coef$fixed),rep(0,6),as.numeric(t(PQL$coef$random$team)))
Q.start<-as.numeric(VarCorr(PQL)[1,1])

for(j in 1:length(lambda))
{
  print(paste("Iteration ", j,sep=""))
  
  glm1 <- try(glmmLasso(points~transfer.spendings  
                        + ave.unfair.score + ball.possession
                        + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                        family = family, data = soccer, lambda=lambda[j],switch.NR=T,final.re=TRUE,
                        control=list(start=Delta.start,q_start=Q.start)), silent=TRUE)  
  
  
  if(class(glm1)!="try-error")
  {  
    BIC_vec[j]<-glm1$bic
  }
  
}

opt<-which.min(BIC_vec)

glm1_final <- glmmLasso(points~transfer.spendings  
                        + ave.unfair.score + ball.possession
                        + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                        family = family, data = soccer, lambda=lambda[opt],switch.NR=F,final.re=TRUE,
                        control=list(start=Delta.start,q_start=Q.start))



summary(glm1_final)





################## Second Simple Method ###########################
## Using 5-fold CV to determine the optimal tuning parameter lambda

### set seed
set.seed(123)
N<-dim(soccer)[1]
ind<-sample(N,N)
lambda <- seq(500,0,by=-5)

kk<-5
nk <- floor(N/kk)

Devianz_ma<-matrix(Inf,ncol=kk,nrow=length(lambda))

## first fit good starting model
library(MASS);library(nlme)
PQL<-glmmPQL(points~1,random = ~1|team,family=family,data=soccer)
Delta.start<-c(as.numeric(PQL$coef$fixed),rep(0,6),as.numeric(t(PQL$coef$random$team)))
Q.start<-as.numeric(VarCorr(PQL)[1,1])


for(j in 1:length(lambda))
{
  print(paste("Iteration ", j,sep=""))
  
  for (i in 1:kk)
  {
    if (i < kk)
    {
      indi <- ind[(i-1)*nk+(1:nk)]
    }else{
      indi <- ind[((i-1)*nk+1):N]
    }
    
    soccer.train<-soccer[-indi,]
    soccer.test<-soccer[indi,]
    
    glm2 <- try(glmmLasso(points~transfer.spendings  
                          + ave.unfair.score  + ball.possession
                          + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                          family = family, data = soccer.train, lambda=lambda[j],switch.NR=F,final.re=TRUE,
                          control=list(start=Delta.start,q_start=Q.start))
                ,silent=TRUE) 
    
    if(class(glm2)!="try-error")
    {  
      y.hat<-predict(glm2,soccer.test)    
      
      Devianz_ma[j,i]<-sum(family$dev.resids(soccer.test$points,y.hat,wt=rep(1,length(y.hat))))
    }
  }
  print(sum(Devianz_ma[j,]))
}

Devianz_vec<-apply(Devianz_ma,1,sum)
opt2<-which.min(Devianz_vec)


glm2_final <- glmmLasso(points~transfer.spendings  
                        + ave.unfair.score + ball.possession
                        + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                        family = family, data = soccer, lambda=lambda[opt2],switch.NR=F,final.re=TRUE,
                        control=list(start=Delta.start,q_start=Q.start))



summary(glm2_final)


################## More Elegant Method ############################################
## Idea: start with big lambda and use the estimates of the previous fit (BUT: before
## the final re-estimation Fisher scoring is performed!) as starting values for the next fit;
## make sure, that your lambda sequence starts at a value big enough such that all covariates are
## shrinked to zero;

## Using BIC (or AIC, respectively) to determine the optimal tuning parameter lambda
lambda <- seq(500,0,by=-5)


BIC_vec<-rep(Inf,length(lambda))
family = poisson(link = log)

# specify starting values for the very first fit; pay attention that Delta.start has suitable length! 
Delta.start<-as.matrix(t(rep(0,7+23)))
Q.start<-0.1  

for(j in 1:length(lambda))
{
  print(paste("Iteration ", j,sep=""))
  
  glm3 <- glmmLasso(points~1 +transfer.spendings
                    + ave.unfair.score 
                    + tackles  
                    + sold.out 
                    + ball.possession
                    + ave.attend
                    ,rnd = list(team=~1),  
                    family = family, data = soccer, 
                    lambda=lambda[j], switch.NR=F,final.re=TRUE,
                    control = list(start=Delta.start[j,],q_start=Q.start[j]))  
  
  print(colnames(glm3$Deltamatrix)[2:7][glm3$Deltamatrix[glm3$conv.step,2:7]!=0])
  BIC_vec[j]<-glm3$bic
  Delta.start<-rbind(Delta.start,glm3$Deltamatrix[glm3$conv.step,])
  Q.start<-c(Q.start,glm3$Q_long[[glm3$conv.step+1]])
}

opt3<-which.min(BIC_vec)

glm3_final <- glmmLasso(points~transfer.spendings + ave.unfair.score 
                        + ball.possession
                        + tackles + ave.attend + sold.out, rnd = list(team=~1),  
                        family = family, data = soccer, lambda=lambda[opt3],
                        switch.NR=F,final.re=TRUE,
                        control = list(start=Delta.start[opt3,],q_start=Q.start[opt3]))  


summary(glm3_final)

## plot coefficient paths
par(mar=c(6,6,4,4))
plot(lambda,Delta.start[2:(length(lambda)+1),2],type="l",ylim=c(-1e-1,1e-1),ylab=expression(hat(beta[j])))
lines(c(-1000,1000),c(0,0),lty=2)
for(i in 3:7){
  lines(lambda[1:length(lambda)],Delta.start[2:(length(lambda)+1),i])
}
abline(v=lambda[opt3],lty=2)
```

# Questions for Laura

1. How many Lambda values to check?
2. Is it okay to base lambda choice on BIC for now? 
  - Should we use the best lambda within 1 SD of the lowest BIC?

# Questions for Laurel and Emily

1. How should we combine levels of race and parent education? White vs. non-white? Above a bachelor's degree vs. bachelor's and below?
