---
title: "JDRFCAV MS 2"
author: "Casey Sakamoto"
date: "2023-04-11"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
#library(unix)
library(tidyverse)
library(tidymodels)
library(doParallel)
library(DataExplorer)
library(knitr)
library(ranger)
library(RANN)

load("S:/Laura/BDC/Projects/Howard Davidson/R01/Data_Raw/JDRFCAV_preprocessed_transformed2.Rdata")

cv = trainControl(method = "cv", number = 5,allowParallel = T)

```

KNN (k = 17) was used to impute data on 14 subjects with missing data. 

```{r knn, include = FALSE}
# make a imputing set
small_subset = transformed_df
small_subset = small_subset %>% mutate(y = cpep_model_decayrate) %>% select(-cpep_model_decayrate)

preProcess_missingdata_model = preProcess(as.data.frame(small_subset), method='knnImpute', k = 17)
# preProcess_missingdata_model

analysis = predict(preProcess_missingdata_model, newdata = small_subset)

# no formula in recipe
analysis_recipe = recipe(analysis)%>%  update_role(everything()) %>% update_role(y, new_role = "outcome")
analysis_recipe = analysis_recipe %>% step_nzv(all_predictors())

```

# SVM with imputations

Similar model fit results as before, but different importance variables and levels.

```{r svmlc}
set.seed(1017)
# Parallel training
cl <- makePSOCKcluster(detectCores()*0.5)
registerDoParallel(cl)
svmlc <- train(
  analysis_recipe,
  data = analysis,
  method = "svmLinear",
  trControl = cv,
  tuneLength = 100,
  tuneGrid = expand.grid(C = seq(0, 2, length = 20)),
  allowParallel = T
)
stopCluster(cl)
# Get the best models
res2 = svmlc$results
kable(head(res2[order(res2$Rsquared,decreasing = T),],5),row.names = F)

svmlc

impvars = varImp(svmlc)
impvars


impvars_df = impvars$importance %>% as.data.frame() %>% rownames_to_column() %>% arrange(desc(Overall))
# impvars$importance %>% 
#   as.data.frame() %>%
#   rownames_to_column() %>%
#   arrange(Overall) %>%
#   mutate(rowname = forcats::fct_inorder(rowname )) %>%
#   ggplot()+
#     geom_col(aes(x = rowname, y = Overall))+
#     coord_flip()+
#     theme_bw()
```

Now that we have an ordered list of variables by model importance, we will build many models with increasing # of variables to examine whether MSE plateaus at a certain number of variables included.

```{r mse fun, echo = F, include = F}
# function to take in a number of variables to pull from impvars df
# create a recipe and build a model 
# return a MSE

# this will exclusively use the analysis dataset and impvars df created above
MSE_fun = function(nvar){
  # recipe step
  df = analysis %>% select(y, impvars_df$rowname[1:nvar])
  recipe = recipe(df)%>%  update_role(everything()) %>% update_role(y, new_role = "outcome") %>% step_nzv(all_predictors())
  
  # fit a model and pull the MSE
  set.seed(1017)
  # Parallel training
  cl <- makePSOCKcluster(detectCores()*0.5)
  registerDoParallel(cl)
  svm <- train(
  recipe,
  data = df,
  method = "svmLinear",
  trControl = cv,
  allowParallel = T)
  stopCluster(cl)

  return(tibble(numvar = nvar, mse = (svm$results$RMSE)^2))
}

plotdf = data.frame(numvar=0, mse=NA)
for (i in 1:1000) {
  plotdf = full_join(plotdf, MSE_fun(i))  
}

```


```{r mse plot}
ggplot(plotdf,aes(x = numvar, y = mse)) + geom_point() + theme_bw()
```