---
title: ""
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
library(arsenal)
library(tidyverse)
library(readxl)
library(performance)
library(knitr) 
library(data.table)
library(broom)
library(pROC)
library(caTools)
library(glmnet)
# analysis pckgs
library(Seurat) # need these for Differential expression testing
library(SeuratObject)
library(DESeq2)
#library(scran)
library(pzfx)
library(SingleCellExperiment)
library(S4Vectors)
# cell express %
#library(scCustomize)

###
#library(cowplot)
#library(Matrix.utils)
#library(edgeR)
library(Matrix)
library(reshape2)
library(S4Vectors)
library(SingleCellExperiment)
library(pheatmap)
#library(apeglm)
library(png)
library(DESeq2)
library(RColorBrewer)
library(data.table)
#library(Matrix.utils)
###

# data
home_dir = "S:/Laura/BDC/Projects/Howard Davidson/R01/Data_Raw/Davidson pilot data/"
knitr::opts_knit$set(root.dir = home_dir)
#setwd("S:/Laura/BDC/Projects/Howard Davidson/R01/Data_Raw/Davidson pilot data/")
# cd4_processed = readRDS(paste0(home_dir, "OneDrive_1_6-28-2023/cd4_cells_processed.rds"))
# cd8_processed = readRDS(paste0(home_dir, "OneDrive_1_6-28-2023/cd8_cells_processed.rds"))
df = readRDS(paste0(home_dir, "OneDrive_1_6-28-2023/normalized.rds"))
#use regular clusters
#df2 = df

# new clusters
#df2 = SetIdent(df, value= df@meta.data$RNA_clustifyr_celltype_individual)
#rm(df)
```

```{r, include = F}
# DE function
# get gene ids
gene_list = unique(rownames(df))

# find cells where most highly expressed
cells = FetchData(df, layer = "counts", vars = gene_list[1:1000]) # whole thing is 15 gigs come back and change later
cells$type = df@active.ident # what is the general type mean? ident? or is there a cell id that im lookn for?
cell_count = data.frame(table(cells$type))
colnames(cell_count) = c("Cell.Type", "n Cells")
cells = cells %>%
    group_by(type) %>%
    summarise(across(where(is.numeric), ~ sum(.x, na.rm = TRUE))) %>%
    column_to_rownames("type")
  max <- lapply(cells, function(c) {
    rownames(cells)[order(c, decreasing = T)[1:2]] })

# DE function
de <- function(seurat_object = so, outcome, ref_group, vars) {
  # Get Entrez Gene IDs
  entrez <- analytes$EntrezGeneSymbol[match(vars, analytes$AptName)]
  entrez <- entrez[entrez %in% rownames(seurat_object)]
  entrez <- unique(entrez)
  # Find cells where most highly expressed
  cells <- FetchData(seurat_object, vars = entrez, slot = "counts")
  cells$type <- seurat_object$generaltype
  cell_count <- data.frame(table(cells$type))
  colnames(cell_count) <- c("Cell.Type", "n Cells")
  cells <- cells %>%
    group_by(type) %>%
    summarise(across(where(is.numeric), ~ sum(.x, na.rm = TRUE))) %>%
    column_to_rownames("type")
  max <- lapply(cells, function(c) {
    rownames(cells)[order(c, decreasing = T)[1:2]]
  })
  # DE testing
  de_tables <- lapply(names(max), function(n) {
    # Compare expression in two highest expressing cell types
    c1 <- tryCatch(
      FindMarkers(seurat_object,
        features = n, ident.1 = ref_group,
        group.by = outcome, subset.ident = max[[n]][1],
        logfc.threshold = 0, verbose = F
      ),
      error = function(err) NULL, warning = function(war) NULL
    )
    if (!is.null(c1)) {
      c1$`Cell Type` <- max[[n]][1]
      c1$`Gene` <- n
    }
    c2 <- tryCatch(
      FindMarkers(seurat_object,
        features = n, ident.1 = ref_group,
        group.by = outcome, subset.ident = max[[n]][2],
        logfc.threshold = 0, verbose = F
      ),
      error = function(err) NULL, warning = function(war) NULL
    )
    if (!is.null(c2)) {
      c2$`Cell Type` <- max[[n]][2]
      c2$`Gene` <- n
    }
    # Merge tables if necessary
    t <- do.call(rbind, list(c1, c2))
    return(t)
  })
  # Combine rows and print
  de_table <- data.frame(do.call(rbind, de_tables))
  de_table <- de_table[order(de_table$Gene), ]
  # Make our own dot plots because Seurat is weird - according to GitHub need to
  # use their dotplot function to get percent expression though? You'd think
  # they would have a function for that...
  dp <- apply(de_table, 1, function(r) {
    g <- r["Gene"]
    i <- r["Cell.Type"]
    d <- DotPlot(seurat_object, features = g, group.by = outcome, idents = i)
    d <- d$data
    d$cell <- i
    return(d)
  })
  dp <- data.frame(do.call(rbind, dp))
  # Default in Seurat is to plot scaled expression, ask Petter which he prefers
  p <- ggplot(dp, aes(x = cell, y = id, color = avg.exp)) +
    geom_point(aes(size = pct.exp)) +
    facet_wrap(~features.plot, scales = "free_x") +
    scale_size_continuous(name = "Percent Expression") +
    scale_color_continuous(name = "Average Expression") +
    theme_bw() +
    theme(axis.title.y = element_blank(), axis.title.x = element_blank())
  cat("\n")
  print(p)
  cat("\n")
  de_table <- de_table %>%
    select(Gene, Cell.Type, avg_log2FC, pct.1, pct.2, p_val, p_val_adj)
  de_table <- left_join(de_table, cell_count, by = join_by(Cell.Type))
  rownames(de_table) <- NULL
  cat("\n")
  pander(de_table)
  cat("\n")
}
# trying to follow the logic
# get the ids
  
```

```{r, include = FALSE}
# outcomes data
read_excel_allsheets <- function(filename, tibble = FALSE) {
    # I prefer straight data.frames
    # but if you like tidyverse tibbles (the default with read_excel)
    # then just pass tibble = TRUE
    sheets <- readxl::excel_sheets(filename)
    x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
    if(!tibble) x <- lapply(x, as.data.frame)
    names(x) <- sheets
    x
}


a1ccpep = read_excel_allsheets("S:/Laura/BDC/Projects/Howard Davidson/R01/Data_Raw/Davidson pilot data/IDDA1C_Cpep.xlsx")
# remove all df as it looks incomplete
a1ccpep = a1ccpep[-1]

# change to numeric
a1c_df <- Reduce(function(x, y) merge(x, y, all = TRUE), a1ccpep)
a1c_df = a1c_df %>% filter(!is.na(`HWD ID`)) %>% mutate(a1c = as.numeric( case_when(A1c_Value == "NULL" ~ NA,
                                                                        TRUE ~ A1c_Value))) ; rm(a1ccpep)
# add in outcome vars
# We are interested in delta, auc, not so much mean 
a1c_df = a1c_df %>% arrange(`HWD ID`, as.Date(VisitDate)) %>% group_by(`HWD ID`) %>% mutate(visit = row_number()) # visit num
a1c_df = data.table(a1c_df, key = '`HWD ID`')

# delta
a1c_df_delta = a1c_df %>% filter(!is.na(a1c)) %>% group_by(`HWD ID`) %>%  mutate(duration_current_wk = as.numeric(difftime(as.Date(VisitDate), as.Date(`Date of Diagnosis`), units = "weeks")),
                                                   v_0y = ifelse(duration_current_wk == min(duration_current_wk), 1, 0),
                                                   v_2y = ifelse(abs(duration_current_wk-104) == min(abs(duration_current_wk - 104)), 1, 0))

# take the y0 and y2 dates and subtract a1c / time interval
delta_df = a1c_df_delta %>% filter(v_0y == 1 | v_2y == 1)
delta_df$v_2y = factor(delta_df$v_2y, labels = c("Y0", "Y2"))
delta_a1c = spread(delta_df%>% select(`HWD ID`, a1c, v_2y), v_2y, a1c)
delta_wks = spread(delta_df%>% select(`HWD ID`, duration_current_wk, v_2y), v_2y, duration_current_wk) %>% mutate(Y0_t = Y0, Y2_t = Y2) %>% select(`HWD ID`, Y0_t, Y2_t)
delta_df = full_join(delta_a1c, delta_wks); rm(delta_wks, delta_a1c, a1c_df_delta)
# calculate delta; this is curently in weeks so well need to divide by 52 to get it into terms of years
delta_df = delta_df %>% mutate(delta_a1c = (Y2- Y0)/(Y2_t/52 - Y0_t/52)) %>% select(`HWD ID`, delta_a1c)
a1c_df = left_join(a1c_df, delta_df)


a1c_df = a1c_df[, days_from_v1 :=  cumsum(c(0, diff(as.Date(VisitDate)))), by=`HWD ID`]# days from baseline

a1c_auc_df = a1c_df %>% group_by(`HWD ID`) %>% filter(!is.na(a1c)) %>% mutate(auc_trap = trapz(days_from_v1, a1c))
a1c_df = left_join(a1c_df, a1c_auc_df)
# fill in values
a1c_df = a1c_df %>% group_by(`HWD ID`) %>% fill(auc_trap)

a1c_df = a1c_df %>% mutate(id = `HWD ID`)

# dichotomize
a1c_df = a1c_df %>% mutate(auc_a1c_g_med = ifelse(auc_trap > median(auc_trap,na.rm = T), "G_med", " L_med"),
                           delta_a1c_g_med = ifelse(delta_a1c > median(delta_a1c,na.rm = T), "G_med", " L_med"))

# add donor id for below
id_df = df2@meta.data %>% select(donor_number, donor) %>% unique()

a1c_df = left_join(a1c_df, id_df, by=c('HWD ID'= "donor_number"))

# IDDA1c

# We are interested in delta, auc, not so much mean 
# delta
idda1c_df_delta = a1c_df %>% filter(!is.na(a1c)) %>% group_by(`HWD ID`) %>%  mutate(duration_current_wk = as.numeric(difftime(as.Date(VisitDate), as.Date(`Date of Diagnosis`), units = "weeks")),
                                                   v_0y = ifelse(duration_current_wk == min(duration_current_wk), 1, 0),
                                                   v_2y = ifelse(abs(duration_current_wk-104) == min(abs(duration_current_wk - 104)), 1, 0))

# take the y0 and y2 dates and subtract a1c / time interval
delta_df = idda1c_df_delta %>% filter(v_0y == 1 | v_2y == 1)
delta_df$v_2y = factor(delta_df$v_2y, labels = c("Y0", "Y2"))
delta_idda1c = spread(delta_df%>% select(`HWD ID`, IDDA1C, v_2y), v_2y, IDDA1C)

### CONFIRM THAT THIS IS DOING WHAT WE WANT FOR ALL SUBJ

delta_wks = spread(delta_df%>% select(`HWD ID`, duration_current_wk, v_2y), v_2y, duration_current_wk) %>% mutate(Y0_t = Y0, Y2_t = Y2) %>% select(`HWD ID`, Y0_t, Y2_t)
delta_df = full_join(delta_a1c, delta_wks); rm(delta_wks, delta_a1c, a1c_df_delta)
# calculate delta; this is curently in weeks so well need to divide by 52 to get it into terms of years
delta_df = delta_df %>% mutate(delta_a1c = (Y2- Y0)/(Y2_t/52 - Y0_t/52)) %>% select(`HWD ID`, delta_a1c)
a1c_df = left_join(a1c_df, delta_df)


a1c_df = a1c_df[, days_from_v1 :=  cumsum(c(0, diff(as.Date(VisitDate)))), by=`HWD ID`]# days from baseline

a1c_auc_df = a1c_df %>% group_by(`HWD ID`) %>% filter(!is.na(a1c)) %>% mutate(auc_trap = trapz(days_from_v1, a1c))
a1c_df = left_join(a1c_df, a1c_auc_df)
# fill in values
a1c_df = a1c_df %>% group_by(`HWD ID`) %>% fill(auc_trap)



df2@meta.data = left_join(df2@meta.data, avg_a1c,  by=c('donor_number'='id'))
# View(df2@meta.data)

# add in other outcomes
# IDDA1c
avg_idda1c = a1c_df %>% group_by(`HWD ID`) %>% summarise(mean_idda1c = mean(IDDA1C, na.rm = T)) %>% mutate(id = `HWD ID`)
# add in slopes and auc
idda1c_slopes = a1c_df %>% group_by(`HWD ID`) %>% do(tidy(lm(IDDA1C ~ days_from_v1, data = .))) %>% filter(term == "days_from_v1") %>% 
  select(`HWD ID`, estimate) %>% rename(idda1c_slope = estimate)
avg_idda1c = left_join(avg_idda1c, idda1c_slopes) ; rm(idda1c_slopes)

avg_idda1c$idda1c_auc = sapply(split(a1c_df,a1c_df$`HWD ID`),function(df) trapz(df$days_from_v1,df$IDDA1C))

# dichotomize
avg_idda1c = avg_idda1c %>% mutate(avg_idda1c_g_med = ifelse(mean_idda1c > median(mean_idda1c,na.rm = T), "G_med", " L_med"),
                             slope_idda1c_g_med = ifelse(idda1c_slope > median(idda1c_slope,na.rm = T), "G_med", " L_med"),
                             auc_idda1c_g_med = ifelse(idda1c_auc > median(idda1c_auc,na.rm = T), "G_med", " L_med"))

df2@meta.data = left_join(df2@meta.data, avg_idda1c,  by=c('donor_number'='id'))


# total daily insulin dose
a1c_df$dose_udk = a1c_df$`U/day/kg`
avg_dose_udk = a1c_df %>% group_by(`HWD ID`) %>% summarise(mean_dose_udk = mean(dose_udk, na.rm = T)) %>% mutate(id = `HWD ID`)
# add in slopes and auc
dose_udk_slopes = a1c_df %>% group_by(`HWD ID`) %>% do(tidy(lm(dose_udk ~ days_from_v1, data = .))) %>% filter(term == "days_from_v1") %>% 
  select(`HWD ID`, estimate) %>% rename(dose_udk_slope = estimate)
avg_dose_udk = left_join(avg_dose_udk, dose_udk_slopes) ; rm(dose_udk_slopes)

avg_dose_udk$dose_udk_auc = sapply(split(a1c_df,a1c_df$`HWD ID`),function(df) trapz(df$days_from_v1,df$dose_udk))

# dichotomize
avg_dose_udk = avg_dose_udk %>% mutate(avg_dose_udk_g_med = ifelse(mean_dose_udk > median(mean_dose_udk,na.rm = T), "G_med", " L_med"),
                             slope_dose_udk_g_med = ifelse(dose_udk_slope > median(dose_udk_slope,na.rm = T), "G_med", " L_med"),
                             auc_dose_udk_g_med = ifelse(dose_udk_auc > median(dose_udk_auc,na.rm = T), "G_med", " L_med"))

df2@meta.data = left_join(df2@meta.data, avg_dose_udk,  by=c('donor_number'='id'))

# cpep
a1c_df = a1c_df %>% mutate(est_cpep = ifelse(`Est C-pep` > 0, `Est C-pep`, 0))
avg_est_cpep = a1c_df %>% group_by(`HWD ID`) %>% summarise(mean_est_cpep = mean(est_cpep, na.rm = T)) %>% mutate(id = `HWD ID`)
# add in slopes and auc
est_cpep_slopes = a1c_df %>% group_by(`HWD ID`) %>% do(tidy(lm(est_cpep ~ days_from_v1, data = .))) %>% filter(term == "days_from_v1") %>% 
  select(`HWD ID`, estimate) %>% rename(est_cpep_slope = estimate)
avg_est_cpep = left_join(avg_est_cpep, est_cpep_slopes) ; rm(est_cpep_slopes)

avg_est_cpep$est_cpep_auc = sapply(split(a1c_df,a1c_df$`HWD ID`),function(df) trapz(df$days_from_v1,df$est_cpep))

# dichotomize
avg_est_cpep = avg_est_cpep %>% mutate(avg_est_cpep_g_med = ifelse(mean_est_cpep > median(mean_est_cpep,na.rm = T), "G_med", " L_med"),
                             slope_est_cpep_g_med = ifelse(est_cpep_slope > median(est_cpep_slope,na.rm = T), "G_med", " L_med"),
                             auc_est_cpep_g_med = ifelse(est_cpep_auc > median(est_cpep_auc,na.rm = T), "G_med", " L_med"))

df2@meta.data = left_join(df2@meta.data, avg_est_cpep,  by=c('donor_number'='id'))
```

```{r pseudobulk, include=FALSE}
# code modified from seurat website

# Extract raw counts and metadata to create SingleCellExperiment object
counts <- df2@assays$RNA@counts 

metadata <- df2@meta.data

# Set up metadata as desired for aggregation and DE analysis
metadata$cluster_id <- factor(df2@active.ident)

# Create single cell experiment object
sce <- SingleCellExperiment(assays = list(counts = counts), 
                           colData = metadata)

# Explore the raw counts for the dataset

## Check the assays present
# assays(sce)

## Check the counts matrix
# dim(counts(sce))
# counts(sce)[1:6, 1:6]

# # Explore the cellular metadata for the dataset
# 
# dim(colData(sce))
# head(colData(sce))

# Extract unique names of clusters (= levels of cluster_id factor variable)
cluster_names <- levels(colData(sce)$cluster_id)
# cluster_names
# 
# # Total number of clusters = 13
# length(cluster_names)

# Extract unique names of samples (= levels of sample_id factor variable)
 sample_names <- levels(colData(sce)$donor)
# sample_names
# 
# # Total number of samples
# length(sample_names)

# Subset metadata to include only the variables you want to aggregate across (here, we want to aggregate by sample and by cluster)
groups <- colData(sce)[, c("cluster_id", "donor")]
head(groups)

# Aggregate across cluster-sample groups
# transposing row/columns to have cell_ids as row names matching those of groups
aggregate.Matrix<-function(x,groupings=NULL,form=NULL,fun='sum',...)
{
  if(!is(x,'Matrix'))
    x<-Matrix(as.matrix(x),sparse=TRUE)
  if(fun=='count')
    x<-x!=0
  groupings2<-groupings
  if(!is(groupings2,'data.frame'))
    groupings2<-as(groupings2,'data.frame')
  groupings2<-data.frame(lapply(groupings2,as.factor))
  groupings2<-data.frame(interaction(groupings2,sep = '_'))
  colnames(groupings2)<-'A'
  if(is.null(form))
    form<-as.formula('~0+.')
  form<-as.formula(form)
  mapping<-dMcast(groupings2,form)
  colnames(mapping)<-substring(colnames(mapping),2)
  result<-t(mapping) %*% x
  if(fun=='mean')
    result@x<-result@x/(aggregate.Matrix(x,groupings2,fun='count'))@x
  attr(result,'crosswalk')<-grr::extract(groupings,match(rownames(result),groupings2$A))
  return(result)
}

#remotes::install_github("cvarrichio/Matrix.utils")
aggr_counts <- aggregate.Matrix(t(counts(sce)), 
                                groupings = groups, fun = "sum") 

# Explore output matrix
# class(aggr_counts)
# dim(aggr_counts)
# aggr_counts[1:6, 1:6]
# Transpose aggregated matrix to have genes as rows and samples as columns
aggr_counts <- t(aggr_counts)
# aggr_counts[1:6, 1:6]

## Exploring structure of function output (list)
#tstrsplit(colnames(aggr_counts), "_") %>% str()

## Comparing the first 10 elements of our input and output strings
# head(colnames(aggr_counts), n = 10)
# head(tstrsplit(colnames(aggr_counts), "_")[[1]], n = 10)

# Loop over all cell types to extract corresponding counts, and store information in a list


# cluster level
## Initiate empty list
counts_ls <- list()

for (i in 1:length(cluster_names)) {

  ## Extract indexes of columns in the global matrix that match a given cluster
  column_idx <- which(tstrsplit(colnames(aggr_counts), "_")[[1]] == cluster_names[i])
  
  ## Store corresponding sub-matrix as one element of a list
  counts_ls[[i]] <- aggr_counts[, column_idx]
  names(counts_ls)[i] <- cluster_names[i]

}

# Explore the different components of the list
str(counts_ls)

# sample level
# Reminder: explore structure of metadata
#head(colData(sce))

# Extract sample-level variables
metadata <- colData(sce) %>% 
  as.data.frame() %>% 
  dplyr::select( donor, donor_number)
 
# dim(metadata)
# head(metadata)

# Exclude duplicated rows
metadata <- metadata[!duplicated(metadata), ]
rownames(metadata) <- metadata$donor
# dim(metadata)
# head(metadata)


# Number of cells per sample and cluster
# t <- table(colData(sce)$donor,
#            colData(sce)$cluster_id)
# t[1:6, 1:6]

# Creating metadata list

## Initiate empty list
metadata_ls <- list()

for (i in 1:length(counts_ls)) {
  
    ## Initiate a data frame for cluster i with one row per sample (matching column names in the counts matrix)
    df_pb <- data.frame(cluster_sample_id = colnames(counts_ls[[i]]))
    
    ## Use tstrsplit() to separate cluster (cell type) and sample IDs
    df_pb$cluster_id <- tstrsplit(df_pb$cluster_sample_id, "_")[[1]]
    df_pb$donor  <- tstrsplit(df_pb$cluster_sample_id, "_")[[2]]
    
    
    ## Retrieve cell count information for this cluster from global cell count table
    idx <- which(colnames(t) == unique(df_pb$cluster_id))
    cell_counts <- t[, idx]
    
    ## Remove samples with zero cell contributing to the cluster
    cell_counts <- cell_counts[cell_counts > 0]
    
    ## Match order of cell_counts and sample_ids
    sample_order <- match(df_pb$donor, names(cell_counts))
    cell_counts <- cell_counts[sample_order]
    
    ## Append cell_counts to data frame
    df_pb$cell_count <- cell_counts
    
    
    ## Join data frame (capturing metadata specific to cluster) to generic metadata
    df_pb <- plyr::join(df_pb, metadata, 
                     by = intersect(names(df_pb), names(metadata)))
    
    ## Update rownames of metadata to match colnames of count matrix, as needed later for DE
    rownames(df_pb) <- df_pb$cluster_sample_id
    
    ## Store complete metadata for cluster i in list
    metadata_ls[[i]] <- df_pb
    names(metadata_ls)[i] <- unique(df_pb$cluster_id)

}

# Explore the different components of the list
#str(metadata_ls)


# Select cell type of interest
# cluster_names
# Double-check that both lists have same names
all(names(counts_ls) == names(metadata_ls))


# Check matching of matrix columns and metadata rows
cluster_counts <- counts_ls[[8]]
cluster_metadata <- metadata_ls[[8]]

cluster_counts[1:6, 1:6]
head(cluster_metadata)

all(colnames(cluster_counts) == rownames(cluster_metadata))
###################################################
# this is the part that im not sure is doing what we want it to do
# Create DESeq2 object        
dds <- DESeqDataSetFromMatrix(cluster_counts, 
                              colData = cluster_metadata, 
                              design = ~ 1)

###################################################

# i think we want a matrix/dataframe where each matrix is combined and added a cluster variable?
counts_lst = lapply(counts_ls, t)
counts_df = lapply(counts_lst, as.data.frame)

new_idcol = function(dfd){
  a = dfd
  a = a %>% mutate(newid = rownames(a),
                   donor = tstrsplit(newid, "_")[[2]],
                   cluster = tstrsplit(newid, "_")[[1]])
  return(a)
}

# move rownames to id/cluster ids
counts_df = lapply(counts_df, new_idcol)

df_binder = function(listy){
  a = listy[[1]]
  for(i in 2:length(listy)){
    a = rbind(a, listy[[i]])
  }
  return(a)
}

# merge dfs
counts_df = df_binder(counts_df)

# do we want to melt the~24000 variables into one variable?
counts_df_long = melt(setDT(counts_df), id.vars = c("donor", "cluster", "newid"), variable.name = "gene")
counts_df_long = counts_df_long %>% group_by(cluster) %>% mutate(total_counts = sum(value),
                                                                 p_exp_ind = value/total_counts)

counts_df_long = counts_df_long %>% group_by(cluster,gene) %>% mutate(total_value = sum(value),
                                                                            p_exp = total_value/total_counts)

gene_clusters = counts_df_long %>% select(cluster, gene, total_value, total_counts, p_exp) %>% unique()
# top 500 by clusters?
genecluster_xpress_t500 = gene_clusters %>% group_by(cluster) %>% arrange(desc(p_exp))  %>% dplyr::slice(1:5)
```

notes: cluster metadata:

very small cell counts for the clusters 10- 12 (maybe 9 as well)


```{r elasticnet, include=FALSE}
# we have the regular a1c outcomes df, a1c_df (outcomes are a1c delta and a1c auc)
# we have pseudobulked genes by both cluseter and cluster/id in countsdflong and genecluster

# combine datasets? once per cluster?
test_net_df = 

  
  
 ###### from tim ############
small_subset = transformed_df

small_subset = small_subset %>% mutate(y = cpep_model_decayrate) %>% select(-cpep_model_decayrate)
small_subset = small_subset %>% na.omit

# no formula in recipe
small_subset_recipe = recipe(small_subset)%>%  update_role(everything()) %>% update_role(y, new_role = "outcome")
small_subset_recipe = small_subset_recipe %>% step_nzv(all_predictors())
## PCA

# pca_large_subset = 
#   large_subset_recipe %>%
#   step_pca(all_predictors(), num_comp = 2)
cv <- trainControl(method = "cv", number = 5,allowParallel = T)

# Base recipe
# Need to use role updating for large dimension datasets
cpep_recipe <- recipe(df) %>%
  update_role(cpep_model_decayrate,new_role = "outcome") %>%
  update_role(-one_of("cpep_model_decayrate"),new_role = "predictor") %>%
  step_nzv(all_predictors())

# Use caret to train the model

# Parallel training
cl <- makePSOCKcluster(detectCores()*0.5)
registerDoParallel(cl)
elnet <- train(
  small_subset_recipe,
  data = na.omit(small_subset),
  method = "glmnet",
  family = "binomial",
  trControl = cv,
  tuneLength = 10,
  allowParallel = T
)
stopCluster(cl)
# Get the best models
res = elnet$results
kable(head(res[order(res$Accuracy,decreasing = T),],5),row.names = F)






########################
  
  #### SAMPLE CODE #######
pkgs <- list("glmnet", "doParallel", "foreach", "pROC")
lapply(pkgs, require, character.only = T)
registerDoParallel(cores = 4)
 
df1 <- read.csv("Downloads/credit_count.txt")
df2 <- df1[df1$CARDHLDR == 1, ]
set.seed(2017)
n <- nrow(df2)
sample <- sample(seq(n), size = n * 0.5, replace = FALSE)
train <- df2[sample, -1]
test <- df2[-sample, -1]
mdlY <- as.factor(as.matrix(train["DEFAULT"]))
mdlX <- as.matrix(train[setdiff(colnames(df1), c("CARDHLDR", "DEFAULT"))])
newY <- as.factor(as.matrix(test["DEFAULT"]))
newX <- as.matrix(test[setdiff(colnames(df1), c("CARDHLDR", "DEFAULT"))])
# ELASTIC NET WITH 0 < ALPHA < 1
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <- cv.glmnet(mdlX, mdlY, family = "binomial", nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = i)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
md3 <- glmnet(mdlX, mdlY, family = "binomial", lambda = cv3$lambda.1se, alpha = cv3$alpha)
coef(md3)
#(Intercept) -1.434700e+00
#AGE         -8.426525e-04
#ACADMOS      .           
#ADEPCNT      .           
#MAJORDRG     6.276924e-02
#MINORDRG     .           
#OWNRENT     -2.780958e-02
#INCOME      -1.305118e-04
#SELFEMPL     .           
#INCPER      -2.085349e-06
#EXP_INC      .           
#SPENDING     .           
#LOGSPEND    -9.992808e-02
roc(newY, as.numeric(predict(md3, newX, type = "response")))
#Area under the curve: 0.6449

```
