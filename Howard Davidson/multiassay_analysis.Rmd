---
title: "TEDDY Multi-Assay Analysis"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(pheatmap)
library(glmnet)
library(snpStats)
library(knitr)
library(igraph)
library(networkD3)
library(car)
library(performance)
library(caret)
library(parallel)
library(doParallel)
knitr::opts_chunk$set(echo = FALSE)
if(Sys.info()["sysname"] == "Windows"){
  home_dir = "B:/Projects/Howard Davidson/TEDDY data"
} else if (Sys.info()["sysname"] == "Linux"){
  home_dir = "~/UCD/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data"
} else if (Sys.info()["sysname"] == "Darwin"){
  home_dir = "/Volumes/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data"
}
knitr::opts_knit$set(root.dir = home_dir)
rm(home_dir)
```

```{r data}
# Transcriptomics and metabolomics
load("./Data_Clean/time_0.RData")
# SNP data
snps = read.plink("./Data_Raw/TEDDY_MP193_Data/SNP/pnnl.bed",
                  "./Data_Raw/TEDDY_MP193_Data/SNP/pnnl.bim",
                  "./Data_Raw/TEDDY_MP193_Data/SNP/pnnl.fam")
snps_from_bjwr = read.csv("./Data_Clean/snps_from_bjwr.csv",header = F)
snps_from_bjwr = unique(snps_from_bjwr$V1)
# Get phenotypes
pheno = lapply(X, function(d){
  colnames(d) = tolower(colnames(d))
  d[,c("id","y")]
})
pheno = do.call(rbind,pheno)
pheno = unique(pheno)
# Format and combine
snp_ids = data.frame(snps$fam$member)
colnames(snp_ids) = "id"
snp_ids$time = 0
snp_ids$y = pheno$y[match(snp_ids$id,pheno$id)]
snps = data.frame(cbind(snp_ids,data.matrix(snps$genotypes)))
snps = snps[!is.na(snps$y),]
# SNPs from BJWR's paper
snps = snps[,which(colnames(snps) %in% c("id","time","y",snps_from_bjwr))]
snps[,4:ncol(snps)] = lapply(snps[,4:ncol(snps)],function(c){as.numeric(c)-1})
```

```{r functions}
# Clean data (standard pipeline)
data_preprocces = function(df,missing_cutoff = 0.8,cv_cutoff = NULL,
                           log_transform = F,scale = T,
                           outcome = "y",id_cols = c("id","time")){
  # Get predictors
  pred = colnames(df)[which(!(colnames(df) %in% c(outcome,id_cols)))]
  # Predictors with too much missing data
  if(!is.null(missing_cutoff)){
    missing = pred[which(colMeans(is.na(df[,pred]))>=missing_cutoff)]
  } else {missing = c()}
  # Predictors with high CV
  if(!is.null(cv_cutoff)){
    cvs = sapply(df[,pred], function(c){
      sd(c,na.rm = T) / mean(c,na.rm = T)
    })
    high_cv = pred[which(cvs > cv_cutoff)]
  } else {high_cv = list()}
  # Remove columns
  initial_set = setdiff(pred,c(missing,high_cv))
  df = df[,c(id_cols,initial_set,outcome)]
  # Remove rows with all missing
  rows_missing = rowSums(is.na(df[,initial_set]))
  df = df[which(rows_missing < length(initial_set)),]
  # Log transform
  if(log_transform){
    df[,initial_set] = lapply(df[,initial_set],log)
    if(any(df == -Inf)){warning("-Inf present after log transform")}
  }
  # Scale
  if(scale){
    df[,initial_set] = lapply(df[,initial_set],scale)
  }
  return(df)
}
# Cluster variables according to DIFAcTO pipeline.
# If optimize_corr = T, this will override the manual corr_cutoff. Correlation
# cutoff should be optimized for all assays, but does not need to be run every time.
corr_cluster = function(df,parallel_ratio = 0.5,folds = 5,
                        outcome = "y",id_cols = c("id","ID","time"),
                        optimize_corr = T, rseq = seq(0,1,by = 0.01),
                        corr_cutoff = NULL,
                        heatmap = T,heatmap_params = list(show_rownames = T,show_colnames = T),
                        network = T){
  # Get predictors
  pred = setdiff(colnames(df),c(id_cols,outcome))
  # Outcome is binary
  df[,outcome] = factor(df[,outcome])
  # Complete cases
  df = df[complete.cases(df),]
  # Correlations between predictors
  corr_mat = cor(df[,pred],use = "pairwise.complete.obs")
  # Get pairs
  var_cor = corr_mat*lower.tri(corr_mat)
  # Find best correlation cutoff
  if(optimize_corr){
    rs = rseq
    # Parallel
    cl <- makeCluster(detectCores()*parallel_ratio,type = "FORK")
    model_perf = parLapply(cl,rs, function(r){
      # Correlation cutoff
      check_cor = which(abs(var_cor) >= r, arr.ind=TRUE)
      # Convert correlated pairs into graph clusters
      graph_cor = graph.data.frame(check_cor, directed = FALSE)
      # Get names
      groups_cor = split(unique(as.vector(check_cor)), clusters(graph_cor)$membership)
      groups = lapply(groups_cor,FUN=function(list_cor){rownames(var_cor)[list_cor]})
      # Find highest correlation within each group
      # Use logistic model estimate instead of correlation
      best_cor = lapply(groups, function(g){
        assocs = lapply(g, function(v){
          form = as.formula(paste0(outcome,"~",v))
          mod = glm(form,df,family = "binomial")
          coefs = summary(mod)$coefficients
          return(abs(coefs[2,1]))
        })
        return(g[which.max(assocs)])
      })
      best_cor = as.character(best_cor)
      if(length(best_cor)>0){
        # CV with caret
        set.seed(1017)
        cv = trainControl(method = "cv",number = folds)
        # Model
        form = as.formula(paste0(outcome,"~",paste0(best_cor,collapse = "+")))
        # CV
        mod = train(
          form, data = df,
          method = "glm",
          family = "binomial",
          trControl = cv,
          tuneLength = 25
        )
        return(mod$results$Accuracy)
      } else {return(NA)}
    })
    stopCluster(cl)
    opt_r = rs[which.max(model_perf)]
  } else {opt_r = corr_cutoff}
  # final groups
  # Correlation cutoff
  check_cor = which(abs(var_cor) >= opt_r, arr.ind=TRUE)
  # Convert correlated pairs into graph clusters
  graph_cor = graph.data.frame(check_cor, directed = FALSE)
  # Get names
  groups_cor = split(unique(as.vector(check_cor)), clusters(graph_cor)$membership)
  groups = lapply(groups_cor,FUN=function(list_cor){rownames(var_cor)[list_cor]})
  # Find highest correlation within each group
  best_cor = lapply(groups, function(g){
    assocs = lapply(g, function(v){
      form = as.formula(paste0(outcome,"~",v))
      mod = glm(form,df,family = "binomial")
      coefs = summary(mod)$coefficients
      return(abs(coefs[2,1]))
    })
    return(g[which.max(assocs)])
  })
  best_cor = as.character(best_cor)
  # Plots
  if(heatmap){
    heatmap_params[["mat"]] = corr_mat
    heat = do.call(pheatmap,heatmap_params)
  }
  if(network){
    # Interactive plot
    int_plot_df = check_cor
    int_plot_df[,1] = rownames(corr_mat)[int_plot_df[,1]]
    int_plot_df[,2] = colnames(corr_mat)[as.numeric(int_plot_df[,2])]
    # Node DF
    nodes <- data.frame(name = unique(c(int_plot_df[,1], int_plot_df[,2])), 
                        stringsAsFactors = FALSE)
    nodes$id <- 0:(nrow(nodes) - 1)
    # Edges DF
    edges <- int_plot_df %>% data.frame(.) %>%
      left_join(nodes, by = c("row" = "name")) %>%
      select(-row) %>%
      rename(source = id) %>%
      left_join(nodes, by = c("col" = "name")) %>%
      select(-col) %>%
      rename(target = id)
    # Plot parameters
    edges$width <- 1
    # Color by group
    nodes$group = sapply(nodes$name, function(n){
      l = lapply(groups, function(x){
        n %in% x
      })
      as.numeric(which(l == TRUE))
    })
    # Plot
    p = forceNetwork(Links = edges, Nodes = nodes, 
                     Source = "source",
                     Target = "target",
                     NodeID ="name",
                     Group = "group",
                     Value = "width",
                     zoom = TRUE,
                     opacity = 1,
                     height = 720,
                     width = 720)
  }
  if(heatmap & network){
    return(list("best" = best_cor,"r_cutoff" = opt_r,"heatmap" = heat,"network" = p))
  } else if (heatmap & !network){
    return(list("best" = best_cor,"r_cutoff" = opt_r,"heatmap" = heat))
  } else if (!heatmap & network){
    return(list("best" = best_cor,"r_cutoff" = opt_r,"network" = p))
  } else {return("best" = best_cor,"r_cutoff" = opt_r,)}
}
```

# DIFAcTO

## Methods

1. Columns with >= 80% missing data were dropped. 
2. Rows missing all of the retained variables from step 1 were dropped.
3. Remaining variables were scaled prior to analysis.
4. For correlation cutoffs from 0 to 1 (by 0.01), predictors were clustered based on correlation. Within each cluster, the variable with the strongest association with the outcome was selected to enter the model. 
5. Predictive accuracy was used to compare model performance between correlation cutoffs. 
6. Variables selected using the optimal correlation cutoff continued on to the lasso. 
7. A lasso model was fit using the variables selected from previous steps. Using the lambda value that results in minimum error selects 0 predictors. So instead we used the lambda value that results in the largest model that is still within 1SD of the minimum error, and fits without numerical probabilities of 0 or 1 occurring.

# Predictor Clustering

## SNPs

```{r}
snps = data_preprocces(snps,scale = F)
snps_best = corr_cluster(snps)
kable(snps_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r snps_best$r_cutoff` based on predictive accuracy.

## Carotenoids

```{r}
carotenoids = data_preprocces(X$carotenoids)
carotenoids_best = corr_cluster(carotenoids)
kable(carotenoids_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r carotenoids_best$r_cutoff` based on predictive accuracy.

## Fatty acids

```{r}
fa = data_preprocces(X$fatty_acids)
fa_best = corr_cluster(fa)
kable(fa_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r fa_best$r_cutoff` based on predictive accuracy.

## Metabolomics

```{r}
metab = data_preprocces(X$metabolomics)
metab_best = corr_cluster(metab)
kable(metab_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r metab_best$r_cutoff` based on predictive accuracy.

## Negative lipidomics

```{r}
negative = data_preprocces(X$negative_lipidomics,cv_cutoff = 0.2)
negative_best = corr_cluster(negative)
kable(negative_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r negative_best$r_cutoff` based on predictive accuracy.

## Positive lipidomics

```{r}
positive = data_preprocces(X$positive_lipidomics)
positive_best = corr_cluster(positive)
kable(positive_best$best,col.names = "Selected variables")
```

The optimal $\rho$ cutoff was `r positive_best$r_cutoff` based on predictive accuracy.

# Lasso

In addition to the selected variables from the 'omics assays above, all variables from assays with 3 or fewer measures (ascorbic acid, cholesterol, growth, etc.) were included in the lasso. 

```{r}
folds = 5
# All variables selected above
lasso_vars = c(snps_best$best,"ascorbic_acid",carotenoids_best$best,"cholesterol",
               fa_best$best,metab_best$best,
               "height_cm","weight_kg","agemos",negative_best$best,positive_best$best,
               "alphatocopherol","gammatocopherol","vitamin_d")
lasso_form = as.formula(paste0("y~",paste0(lasso_vars,collapse = "+")))
# Pull out selected variables
smaller_dfs = lapply(X, function(d){
  d = d[,which(colnames(d) %in% c("ID","id","time","y",lasso_vars))]
})
# Combine into single dataframe
df = full_join(smaller_dfs[[1]],smaller_dfs[[2]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[3]],by = c("ID","y","time"))
df = full_join(df,smaller_dfs[[4]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[5]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[6]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[7]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[8]],by = c("ID"="id","time","y"))
df = left_join(df,smaller_dfs[[9]],by = c("ID"="id","time","y"))
df[,grep("\\.y",colnames(df))] = NULL
colnames(df)[grep("\\.x",colnames(df))] = 
  sub("\\.x","",colnames(df)[grep("\\.x",colnames(df))])
df = left_join(df,smaller_dfs[[10]],by = c("ID"="id","time","y"))
df = left_join(df,smaller_dfs[[11]],by = c("y", "ID", "time"))
df = left_join(df,snps[,which(colnames(snps) %in% c("ID","id","time","y",lasso_vars))],
               by = c("ID"="id","time","y"))
df$y = as.factor(df$y)
# Remove missing
lasso_df = df[complete.cases(df),]
lasso_df$y = factor(lasso_df$y)
# CV with caret
set.seed(1017)
cv = trainControl(method = "cv",number = folds)
# Model
form = as.formula(paste0("y~",paste0(lasso_vars,collapse = "+")))
# Parallel
cl = makePSOCKcluster(folds)
registerDoParallel(cl)
mod = train(
  form, data = lasso_df,
  method = "glmnet",
  family = "binomial",
  trControl = cv,
  tuneLength = 25
)
stopCluster(cl)
# Best lambda 
selected = coef(mod$finalModel,s = mod$bestTune$lambda)
selected = rownames(selected)[which(selected != 0 & rownames(selected) != "(Intercept)")]
difacto_selected = selected
kable(selected,col.names = "Selected variables")
```

```{r}
# CV on everything
form = as.formula(paste0("y~",paste0(selected,collapse = "+")))
cl = makePSOCKcluster(folds)
registerDoParallel(cl)
mod = train(
  form, data = df,
  method = "glm",
  family = "binomial",
  trControl = cv,
  na.action = na.omit
)
stopCluster(cl)
```

This model with `r length(selected)` markers resulted in a CV accuracy of `r round(mod$results$Accuracy,3)` (accuracy SD `r round(mod$results$AccuracySD,3)`).

# Elasticnet

## Methods

1. Columns with >= 80% missing data were dropped. 
2. Rows missing all of the retained variables from step 1 were dropped.
3. Remaining variables were scaled prior to analysis.
4. Due to missing data problems, an elasticnet was run for each assay separately. 

```{r elasticnet function}
elastic = function(df,folds = 5,outcome = "y",id_cols = c("id","ID","time")){
  # Outcome 
  df[,outcome] = factor(df[,outcome])
  # Predictors
  pred = setdiff(colnames(df),c(id_cols,outcome))
  # Complete
  df = df[complete.cases(df),]
  # Formula
  form = as.formula(paste0(outcome,"~",paste0(pred,collapse = "+")))
  # Caret CV
  set.seed(1017)
  cv = trainControl(method = "cv",number = folds)
  # Parallel
  cl = makePSOCKcluster(folds)
  registerDoParallel(cl)
  mod = train(
    form, data = df,
    method = "glmnet",
    family = "binomial",
    trControl = cv,
    tuneLength = 25
  )
  stopCluster(cl)
  # Best lambda 
  selected = coef(mod$finalModel,s = mod$bestTune$lambda)
  selected = rownames(selected)[which(selected != 0 & rownames(selected) != "(Intercept)")]
  return(selected)
}
```

## SNPs

```{r}
snps_elastic = elastic(snps)
kable(snps_elastic,col.names = "Selected variables")
```

## Carotenoids

```{r}
carot_elastic = elastic(carotenoids)
kable(carot_elastic,col.names = "Selected variables")
```

## Fatty acids

```{r}
fa_elastic = elastic(fa)
kable(fa_elastic,col.names = "Selected variables")
```

## Metabolomics

```{r}
metab_elastic = elastic(metab)
kable(metab_elastic,col.names = "Selected variables")
```

## Negative lipidomics

```{r}
neg_elastic = elastic(negative)
kable(neg_elastic,col.names = "Selected variables")
```

## Positive lipidomics

```{r}
pos_elastic = elastic(positive)
kable(pos_elastic,col.names = "Selected variables")
```

## Elasticnet results

```{r}
# All variables selected above
elastic_vars = c(snps_elastic,"ascorbic_acid",carot_elastic,"cholesterol",
                 fa_elastic,metab_elastic,
                 "height_cm","weight_kg","agemos",neg_elastic,pos_elastic,
                 "alphatocopherol","gammatocopherol","vitamin_d")
elastic_form = as.formula(paste0("y~",paste0(elastic_vars,collapse = "+")))
# Pull out selected variables
smaller_dfs = lapply(X, function(d){
  d = d[,which(colnames(d) %in% c("ID","id","time","y",elastic_vars))]
})
# Combine into single dataframe
df = full_join(smaller_dfs[[1]],smaller_dfs[[2]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[3]],by = c("ID","y","time"))
df = full_join(df,smaller_dfs[[4]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[5]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[6]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[7]],by = c("ID"="id","time","y"))
df = full_join(df,smaller_dfs[[8]],by = c("ID"="id","time","y"))
df = left_join(df,smaller_dfs[[9]],by = c("ID"="id","time","y"))
df[,grep("\\.y",colnames(df))] = NULL
colnames(df)[grep("\\.x",colnames(df))] = 
  sub("\\.x","",colnames(df)[grep("\\.x",colnames(df))])
df = left_join(df,smaller_dfs[[10]],by = c("ID"="id","time","y"))
df = left_join(df,smaller_dfs[[11]],by = c("y", "ID", "time"))
df = left_join(df,snps[,which(colnames(snps) %in% c("ID","id","time","y",elastic_vars))],
               by = c("ID"="id","time","y"))
df$y = factor(df$y)
# CV with caret
set.seed(1017)
cv = trainControl(method = "cv",number = folds)
# Parallel
cl = makePSOCKcluster(folds)
registerDoParallel(cl)
mod = train(
  elastic_form, data = df,
  method = "glm",
  family = "binomial",
  trControl = cv,
  na.action = na.omit
)
stopCluster(cl)
indiv_elastic_selected = rownames(summary(mod)$coefficients)
indiv_elastic_selected = indiv_elastic_selected[indiv_elastic_selected != "(Intercept)"]
kable(summary(mod)$coefficients,digits = 3)
```

This model with `r nrow(summary(mod)$coefficients)-1` markers resulted in a CV accuracy of `r round(mod$results$Accuracy,3)` (accuracy SD `r round(mod$results$AccuracySD,3)`).

# Elasticnet on everything

1. Variables with >50% missing data were excluded to maximize the number of complete case observations.

```{r include=FALSE}
df = full_join(X$carotenoids,X$ascorbic_acid,by = c("id" = "ID","time","y"))
df = full_join(df,X$cholesterol,by = c("id" = "ID","time","y"))
df = full_join(df,X$fatty_acids,by = c("y", "id", "time"))
df = full_join(df,X$metabolomics,by = c("y", "id", "time"))
df = full_join(df,X$metabolomics)
df = full_join(df,X$growth,by = c("y", "id", "time"))
# df = full_join(df,X$negative_lipidomics,by = c("y", "id", "time"))
df = full_join(df,X$positive_lipidomics)
df = full_join(df,X$tocoperol,by = c("y", "id", "time"))
df = full_join(df,X$vitamin_d,by = c("id" = "ID","time","y"))
df = left_join(df,snps,by = c("y", "id", "time"))
# # Remove more than 50% missing
df = data_preprocces(df,missing_cutoff = 0.5,scale = F)
df = df[complete.cases(df),]
```

```{r}
# CV
set.seed(1017)
cv = trainControl(method = "cv",number = folds)
# Outcome is binary
df$y = factor(df$y)
# Parallel
cl = makePSOCKcluster(folds)
registerDoParallel(cl)
mod = train(x = df[,setdiff(colnames(df),c("id","time","y"))],y = df$y,
  method = "glmnet",
  family = "binomial",
  trControl = cv,
  tuneLength = 25,
  preProcess = c("center", "scale")
)
stopCluster(cl)
# Best lambda 
selected = coef(mod$finalModel,s = mod$bestTune$lambda)
selected = rownames(selected)[which(selected != 0 & rownames(selected) != "(Intercept)")]
big_elastic_selected = selected
kable(selected,col.names = "Selected variables")
```

```{r}
# CV on everything
form = as.formula(paste0("y~",paste0(selected,collapse = "+")))
cl = makePSOCKcluster(folds)
registerDoParallel(cl)
mod = train(
  form, data = df,
  method = "glm",
  family = "binomial",
  trControl = cv,
  na.action = na.omit
)
stopCluster(cl)
```

This model with `r length(selected)` markers resulted in a CV accuracy of `r round(mod$results$Accuracy,3)` (accuracy SD `r round(mod$results$AccuracySD,3)`).

```{r}
intersect(intersect(difacto_selected,indiv_elastic_selected),big_elastic_selected)
```
