---
title: "TEDDY Multi-Assay Analysis"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
library(mlr3verse)
library(tidyverse)
library(pheatmap)
library(skimr)
library(glmnet)
library(knitr)
library(igraph)
library(networkD3)
knitr::opts_chunk$set(echo = FALSE)
home_dir = ifelse(.Platform$OS.type != "unix",
                  "Z:/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data",
                  "~/UCD/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data")
knitr::opts_knit$set(root.dir = home_dir)
rm(home_dir)
```

```{r data cleaning,message=FALSE}
load("./Data_Clean/time_0.RData")
```

```{r functions}
# Clean data (standard pipeline)
data_preprocces = function(df,missing_cutoff = 0.8,cv_cutoff = NULL,
                           log_transform = F,scale = T,
                           outcome = "y",id_cols = c("id","time")){
  # Get predictors
  pred = colnames(df)[which(!(colnames(df) %in% c(outcome,id_cols)))]
  # Predictors with too much missing data
  if(!is.null(missing_cutoff)){
    missing = pred[which(colMeans(is.na(df[,pred]))>=missing_cutoff)]
  } else {missing = c()}
  # Predictors with high CV
  if(!is.null(cv_cutoff)){
    cvs = sapply(df[,pred], function(c){
      sd(c,na.rm = T) / mean(c,na.rm = T)
    })
    high_cv = pred[which(cvs > cv_cutoff)]
  } else {high_cv = list()}
  # Remove columns
  initial_set = setdiff(pred,c(missing,high_cv))
  df = df[,c(id_cols,initial_set,outcome)]
  # Remove rows with all missing
  rows_missing = rowSums(is.na(df[,initial_set]))
  df = df[which(rows_missing < length(initial_set)),]
  # Log transform
  if(log_transform){
    df[,initial_set] = lapply(df[,initial_set],log)
    if(any(df == -Inf)){warning("-Inf present after log transform")}
  }
  # Scale
  if(scale){
    df[,initial_set] = lapply(df[,initial_set],scale)
  }
  return(df)
}
# Cluster variables according to DIFAcTO pipeline
corr_cluster = function(df,corr_cutoff = 0.7,core_ratio = 0.5){
  
}

```

# Methods

1. Data were combined into rectangular format with a total of `r length(all_vars)` variables.
2. Columns with >= `r missing_cutoff*100`% missing data or coefficient of variation (CV) > 30% were dropped. There were `r length(missing)` variables with too much missing data, and `r length(high_cv)` with a high CV. A total of `r ncol(time_0)-5` variables remained after this step. 
3. Rows missing all of the retained variables from step 2 were dropped, resulting in `r nrow(time_0)` observations.
4. Remaining variables were scaled prior to analysis.
4. Correlation between the remaining predictors was examined visually using a heatmap, and predictors were clustered based on correlation coefficient. 
5. Within each cluster, the variable with the strongest association with the outcome was selected to continue on to the lasso.

# Predictor Clustering

## By correlation with other predictors

### Heatmap of variable correlations

```{r}
corr_mat = data.frame(cor(time_0[,initial_set],use = "pairwise.complete.obs"))
# Plot
plot_mat = corr_mat
blank = names(which(rowSums(is.na(plot_mat)) == ncol(plot_mat)))
plot_mat[,blank] = NULL
plot_mat = plot_mat[!(row.names(plot_mat) %in% blank),]
pheatmap(plot_mat,show_rownames = F,show_colnames = F)
```

### Variable network diagram

```{r}
# Get pairs
var_cor = corr_mat*lower.tri(corr_mat)
# Correlation
corr_cutoff = 0.7
check_cor = which(abs(var_cor) >= corr_cutoff, arr.ind=TRUE)
# Convert correlated pairs into graph clusters
graph_cor = graph.data.frame(check_cor, directed = FALSE)
# Get names
groups_cor = split(unique(as.vector(check_cor)), clusters(graph_cor)$membership)
groups = lapply(groups_cor,FUN=function(list_cor){rownames(var_cor)[list_cor]})
# Find highest correlation within each group
best_cor = lapply(groups, function(g){
  assocs = lapply(g, function(v){
    form = as.formula(paste0("y~",v))
    mod = glm(form,time_0,family = "binomial")
    coefs = summary(mod)$coefficients
    return(abs(coefs[2,1]))
  })
  return(g[which.max(assocs)])
})
best_cor = as.character(best_cor)
# Interactive plot
int_plot_df = check_cor
int_plot_df[,1] = rownames(corr_mat)[int_plot_df[,1]]
int_plot_df[,2] = colnames(corr_mat)[as.numeric(int_plot_df[,2])]

nodes <- data.frame(name = unique(c(int_plot_df[,1], int_plot_df[,2])), stringsAsFactors = FALSE)
nodes$id <- 0:(nrow(nodes) - 1)

edges <- int_plot_df %>% data.frame(.) %>%
  left_join(nodes, by = c("row" = "name")) %>%
  select(-row) %>%
  rename(source = id) %>%
  left_join(nodes, by = c("col" = "name")) %>%
  select(-col) %>%
  rename(target = id)

edges$width <- 1

nodes$group = sapply(nodes$name, function(n){
  l = lapply(groups, function(x){
    n %in% x
  })
  as.numeric(which(l == TRUE))
})



p = forceNetwork(Links = edges, Nodes = nodes, 
                 Source = "source",
                 Target = "target",
                 NodeID ="name",
                 Group = "group",
                 Value = "width",
                 opacity = 0.9,
                 zoom = TRUE)
p
```

Using a within cluster correlation cutoff of `r corr_cutoff` results in `r length(groups)` clusters. However, some of these clusters contain a large number of variables. 

# Lasso

```{r}
lasso_df = time_0[,c(best_cor,"y")]
lasso_df = data.matrix(lasso_df[complete.cases(lasso_df),])
# Use lambda.min from glmnet.cv
cv = cv.glmnet(x = lasso_df[,1:(ncol(lasso_df)-1)],y = lasso_df[,"y"],family = "binomial")
mod = glm.fit(x = lasso_df[,1:(ncol(lasso_df)-1)],y = lasso_df[,"y"],family = "binomial")
cs = coef(mod)
```

The variables `r paste(best_cor,collapse = ", ")` were selected to continue to the lasso step. `r sum(coef(mod)>0)` were selected by the lasso:

```{r}
kable(rownames(cs)[which(cs[,1]!=0)],col.names = "")
```

# Alternative approaches and questions

1. The DIFAcTO pipeline was not created to analyze longitudinal data. How should we adapt it for TEDDY?
2. Using the DIFAcTO method of creating clusters results in a very small number of variables moving on to the lasso. We could try to optimize the correlation cutoff, or we could cut the cluster tree to create a pre-determined number of variables.
3. The DIFAcTO methos also selects just one of the many correlated variables in a cluster, but this may be losing a huge amount of information, particularly in clusters with a large number of variables. Could we use elasticnet instead? This is an approach that is very similar to the lasso, but can select groups of correlated variables.
4. What known factors should be included in the lasso (e.g. BMI, etc.)?
5. Are there variables that should be combined? 