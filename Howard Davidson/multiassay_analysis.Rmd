---
title: "TEDDY Multi-Assay Analysis"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
library(mlr3verse)
library(tidyverse)
library(pheatmap)
library(skimr)
library(glmnet)
library(knitr)
library(igraph)
library(networkD3)
knitr::opts_chunk$set(echo = FALSE)
home_dir = ifelse(.Platform$OS.type != "unix",
                  "Z:/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data",
                  "~/UCD/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Howard Davidson/TEDDY data")
knitr::opts_knit$set(root.dir = home_dir)
rm(home_dir)
```

```{r data cleaning,message=FALSE}
load("./Data_Clean/time_0.RData")
```

```{r functions}
# Clean data (standard pipeline)
data_preprocces = function(df,missing_cutoff = 0.8,cv_cutoff = NULL,
                           log_transform = F,scale = T,
                           outcome = "y",id_cols = c("id","time")){
  # Get predictors
  pred = colnames(df)[which(!(colnames(df) %in% c(outcome,id_cols)))]
  # Predictors with too much missing data
  if(!is.null(missing_cutoff)){
    missing = pred[which(colMeans(is.na(df[,pred]))>=missing_cutoff)]
  } else {missing = c()}
  # Predictors with high CV
  if(!is.null(cv_cutoff)){
    cvs = sapply(df[,pred], function(c){
      sd(c,na.rm = T) / mean(c,na.rm = T)
    })
    high_cv = pred[which(cvs > cv_cutoff)]
  } else {high_cv = list()}
  # Remove columns
  initial_set = setdiff(pred,c(missing,high_cv))
  df = df[,c(id_cols,initial_set,outcome)]
  # Remove rows with all missing
  rows_missing = rowSums(is.na(df[,initial_set]))
  df = df[which(rows_missing < length(initial_set)),]
  # Log transform
  if(log_transform){
    df[,initial_set] = lapply(df[,initial_set],log)
    if(any(df == -Inf)){warning("-Inf present after log transform")}
  }
  # Scale
  if(scale){
    df[,initial_set] = lapply(df[,initial_set],scale)
  }
  return(df)
}
# Cluster variables according to DIFAcTO pipeline
corr_cluster = function(df,corr_cutoff = 0.7,parallel_ratio = 0.5,
                        outcome = "y",id_cols = c("id","time"),
                        heatmap = T,heatmap_params = list(show_rownames = F,show_colnames = F),
                        network = T){
  # Get predictors
  pred = setdiff(colnames(df),c(id_cols,outcome))
  # Correlations between predictors
  corr_mat = cor(df[,initial_set],use = "pairwise.complete.obs")
  # Get pairs
  var_cor = corr_mat*lower.tri(corr_mat)
  # Correlation cutoff
  check_cor = which(abs(var_cor) >= corr_cutoff, arr.ind=TRUE)
  # Convert correlated pairs into graph clusters
  graph_cor = graph.data.frame(check_cor, directed = FALSE)
  # Get names
  groups_cor = split(unique(as.vector(check_cor)), clusters(graph_cor)$membership)
  groups = lapply(groups_cor,FUN=function(list_cor){rownames(var_cor)[list_cor]})
  # Find highest correlation within each group
  best_cor = lapply(groups, function(g){
    assocs = lapply(g, function(v){
      form = as.formula(paste0(outcome,"~",v))
      mod = glm(form,df,family = "binomial")
      coefs = summary(mod)$coefficients
      return(abs(coefs[2,1]))
    })
    return(g[which.max(assocs)])
  })
  best_cor = as.character(best_cor)
  # Plots
  if(heatmap){
    heatmap_params[["mat"]] = corr_mat
    heat = do.call(pheatmap,heatmap_params)
  }
  if(network){
    # Interactive plot
    int_plot_df = check_cor
    int_plot_df[,1] = rownames(corr_mat)[int_plot_df[,1]]
    int_plot_df[,2] = colnames(corr_mat)[as.numeric(int_plot_df[,2])]
    # Node DF
    nodes <- data.frame(name = unique(c(int_plot_df[,1], int_plot_df[,2])), stringsAsFactors = FALSE)
    nodes$id <- 0:(nrow(nodes) - 1)
    # Edges DF
    edges <- int_plot_df %>% data.frame(.) %>%
      left_join(nodes, by = c("row" = "name")) %>%
      select(-row) %>%
      rename(source = id) %>%
      left_join(nodes, by = c("col" = "name")) %>%
      select(-col) %>%
      rename(target = id)
    # PLot parameters
    edges$width <- 1
    # Color by group
    nodes$group = sapply(nodes$name, function(n){
      l = lapply(groups, function(x){
        n %in% x
      })
      as.numeric(which(l == TRUE))
    })
    # Plot
    p = forceNetwork(Links = edges, Nodes = nodes, 
                     Source = "source",
                     Target = "target",
                     NodeID ="name",
                     Group = "group",
                     Value = "width",
                     opacity = 0.9,
                     zoom = TRUE)
  }
  if(heatmap & network){
    return(list("best" = best_cor,"heatmap" = heat,"network" = p))
  } else if (heatmap & !network){
    return(list("best" = best_cor,"heatmap" = heat))
  } else if (!heatmap & network){
      return(list("best" = best_cor,"network" = p))
    } else {return(best_cor)}
}
```

# Methods

1. Columns with >= 80% missing data were dropped. 
3. Rows missing all of the retained variables from step 1 were dropped.
4. Remaining variables were scaled prior to analysis.
4. Correlation between the remaining predictors was examined visually using a heatmap, and predictors were clustered based on correlation coefficient. 
5. Within each cluster, the variable with the strongest association with the outcome was selected to continue on to the lasso.

# Predictor Clustering

## Carotenoids

```{r}
d = data_preprocces(X$carotenoids)
res = corr_cluster(d)
res$heatmap
res$network
```

# Lasso

```{r}
lasso_df = time_0[,c(best_cor,"y")]
lasso_df = data.matrix(lasso_df[complete.cases(lasso_df),])
# Use lambda.min from glmnet.cv
cv = cv.glmnet(x = lasso_df[,1:(ncol(lasso_df)-1)],y = lasso_df[,"y"],family = "binomial")
mod = glm.fit(x = lasso_df[,1:(ncol(lasso_df)-1)],y = lasso_df[,"y"],family = "binomial")
cs = coef(mod)
```

The variables `r paste(best_cor,collapse = ", ")` were selected to continue to the lasso step. `r sum(coef(mod)>0)` were selected by the lasso:

```{r}
kable(rownames(cs)[which(cs[,1]!=0)],col.names = "")
```

# Alternative approaches and questions

1. The DIFAcTO pipeline was not created to analyze longitudinal data. How should we adapt it for TEDDY?
2. Using the DIFAcTO method of creating clusters results in a very small number of variables moving on to the lasso. We could try to optimize the correlation cutoff, or we could cut the cluster tree to create a pre-determined number of variables.
3. The DIFAcTO methos also selects just one of the many correlated variables in a cluster, but this may be losing a huge amount of information, particularly in clusters with a large number of variables. Could we use elasticnet instead? This is an approach that is very similar to the lasso, but can select groups of correlated variables.
4. What known factors should be included in the lasso (e.g. BMI, etc.)?
5. Are there variables that should be combined? 