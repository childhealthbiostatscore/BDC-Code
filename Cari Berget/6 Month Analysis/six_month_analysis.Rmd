---
title: "6 Month Analysis"
author: "Tim Vigers & Laura Pyle"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tools)
library(gridExtra)
library(readxl)
library(tableone)
library(nlme)
library(emmeans)
library(knitr)
library(reshape2)
library(tidyverse)
source("/Users/timvigers/Documents/GitHub/Tim-and-Laura/tim_R_functions.R")
# Check OS and alter file path accordingly.
if (.Platform$OS.type == "windows") {pathstart <- "//ucdenver.pvt/"} else if (.Platform$OS.type == "unix"){pathstart <- "/Volumes/"}
```

```{r echo=FALSE,warning=FALSE,eval=FALSE}
# Read in glycemic data
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_GlycemicDATA_2019-01-07_.csv")
glycemicdata <- read.csv(filename,na.strings = "",stringsAsFactors = F)
# Format dates
datecols <- c("demographics_consent","demographics_dob","demographics_diabetesdx","automode_start",
              "hba1c_date_b","hba1c_date_m1","t1_date_m1","hba1c_date_t1","t1_date","hba1c_date_t2",
              "t2_date")
glycemicdata[,datecols] <- lapply(glycemicdata[,datecols], function(x) mdy(x,tz = "MST"))
```

```{r echo=FALSE, eval=FALSE}
# Re-assign visit dates based on Cari's decisions. CSV file manually edited for easier import.
# Notes:
# 1. Cari is double checking #14 dates, others in CSV file are correct. 
# 2. Baseline A1c can be 2 weeks after AM start.
# 3. "Date Questions.csv" manually edited for easier R import.
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/Date Questions.csv")
correct.dates <- read.csv(filename,stringsAsFactors = F,na.strings = c("","none"))
correct.dates[,2:5] <- lapply(correct.dates[,2:5],function(x) mdy(x,tz = "MST"))
# Split into separate data frames, rename columns.
m1cols <- c(grep("m1_",colnames(glycemicdata)),grep("_m1",colnames(glycemicdata)))
original.m1 <- glycemicdata[,c(1,m1cols)]
colnames(original.m1) <- sub("_m1","",colnames(original.m1))
colnames(original.m1) <- sub("t1_","",colnames(original.m1))
# T1
t1cols <- c(grep("t1_",colnames(glycemicdata)),grep("_t1",colnames(glycemicdata)))
original.t1 <- glycemicdata[,c(1,t1cols)]
colnames(original.t1) <- sub("t1_","",colnames(original.t1))
colnames(original.t1) <- sub("_t1","",colnames(original.t1))
# T2
t2cols <- c(grep("t2_",colnames(glycemicdata)),grep("_t2",colnames(glycemicdata)))
original.t2 <- glycemicdata[,c(1,t2cols)]
colnames(original.t2) <- sub("t2_","",colnames(original.t2))
colnames(original.t2) <- sub("_t2","",colnames(original.t2))
# Define variables of interest
vars <- c("hba1c","am_time","mm_time","sensor_wear","sensor_u54","sensor_55_69",
          "sensor_70_180","sensor_181_250","sensor_g250","mean_sg","sd",
          "bg_checks","calibrations","tdd","basal","bolus","amexit",
          "amexit_day","amexit_hyper","amexit_hypo","amexit_manual","amexit_other")
# Combine, remove duplicates and melt
allcols <- c("record_id","date",vars)
alldat <- rbind(original.m1[,allcols],original.t1[,allcols],original.t2[,allcols])
alldat <- alldat[which(duplicated(alldat[,c("record_id","date")])==F),]
alldat <- melt(alldat,id.vars = c("record_id","date"))
# Spread
alldat <- spread(alldat,key = variable,value = value)
# Get corrected M1 data
m1 <- correct.dates[,c("record_id","correct.m1.date")]
colnames(m1) <- c("record_id","date")
m1 <- left_join(m1,alldat,by = c("record_id","date"))
m1$tpoint <- "M1"
# Get corrected T1 data
t1 <- correct.dates[,c("record_id","correct.t1.date")]
colnames(t1) <- c("record_id","date")
t1 <- left_join(t1,alldat,by = c("record_id","date"))
t1$tpoint <- "T1"
# Get corrected T2 data
t2 <- correct.dates[,c("record_id","correct.t2.date")]
colnames(t2) <- c("record_id","date")
t2 <- left_join(t2,alldat,by = c("record_id","date"))
t2$tpoint <- "T2"
# Merge M1, T1, and T2
alldat <- bind_rows(m1,t1,t2)
alldat <- alldat[order(alldat$record_id),]
# Add autmode start and calculate days
alldat <- merge(alldat,glycemicdata[,c("record_id","automode_start")])
alldat$days <- as.numeric(difftime(alldat$date, alldat$automode_start,units = "days"))
# Import baseline data
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_BaselineGlycemicData_14JAN2019.csv")
baseline <- read.csv(filename)
colnames(baseline) <- sub("_b","",colnames(baseline))
baseline$tpoint <- "B"
# Merge everything, sort
alldat <- bind_rows(alldat,baseline)
alldat <- alldat[order(alldat$record_id),]
# Make record ID and timepoint factor
alldat$record_id <- as.factor(alldat$record_id)
alldat$tpoint <- as.factor(alldat$tpoint)
# Get baseline A1c
alldat$hba1c[alldat$tpoint == "B"] <- glycemicdata$hba1c_baseline
```

```{r echo=FALSE,eval=FALSE}
# Read in survey data
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_SurveyDATA_2019-04-26_.csv")
child <- read.csv(filename,na.strings = "",stringsAsFactors = F)
# Check time from AM start
am_start <- read.csv(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_GlycemicDATA_2019-01-07_.csv"))
am_start <- am_start[,c("record_id","automode_start")]
child <- left_join(child,am_start,by="record_id")
child[,c("date","date_t1","c_date_t2","automode_start")] <- 
  lapply(child[,c("date","date_t1","c_date_t2","automode_start")], lubridate::mdy)
# Separate by timepoint,melt
child_baseline <- child %>%
  select(record_id,date,automode_start,c_hfs_behave1:c_paid20) %>%
  filter(!is.na(date),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(date,automode_start,units = "days"))
colnames(child_baseline) <- sub("c_","",colnames(child_baseline))

child_t1 <- child %>%
  select(record_id,date_t1,automode_start,c_hfs_behave1_t1:c_paid20_t1) %>%
  filter(!is.na(date_t1),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(date_t1,automode_start,units = "days"))
colnames(child_t1) <- gsub("c_|_t1","",colnames(child_t1))

child_t2 <- child %>%
  select(record_id,c_date_t2,automode_start,c_hfs_behave1_t2:c_paid20_t2) %>%
  filter(!is.na(c_date_t2),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(c_date_t2,automode_start,units = "days"))
colnames(child_t2) <- gsub("c_|_t2","",colnames(child_t2))

child_long <- rbind(child_baseline,child_t1,child_t2) %>%
  arrange(record_id,date)
child_long$days_from_am <- as.numeric(child_long$days_from_am)
child_long$tpoint <- cut(child_long$days_from_am,breaks = c(-Inf,0,45,135,225,Inf),
                         right = T,labels = c("B","M1","T1","T2","After T2"))
# YA
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GYoungAdult_SurveyDATA_2019-04-26_.csv")
ya <- read.csv(filename,na.strings = "",stringsAsFactors = F)

ya <- left_join(ya,am_start,by="record_id")
ya[,c("ya_hfs_date","ya_hfs_date_t1","ya_hfs_date_t2","automode_start")] <- 
  lapply(ya[,c("ya_hfs_date","ya_hfs_date_t1","ya_hfs_date_t2","automode_start")], lubridate::mdy)
# Separate by timepoint,melt
ya_baseline <- ya %>%
  select(record_id,ya_hfs_date,automode_start,ya_hfs_behave1:ya_hfs_worry18,ya_paid1_base:ya_paid20_base) %>%
  filter(!is.na(ya_hfs_date),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(ya_hfs_date,automode_start,units = "days"))
colnames(ya_baseline) <- gsub("ya_|_base","",colnames(ya_baseline))

ya_t1 <- ya %>%
  select(record_id,ya_hfs_date_t1,automode_start,ya_hfs_behave1_t1:ya_hfs_worry18_t1,ya_paid1_t1:ya_paid20_t1) %>%
  filter(!is.na(ya_hfs_date_t1),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(ya_hfs_date_t1,automode_start,units = "days"))
colnames(ya_t1) <- gsub("ya_|_t1","",colnames(ya_t1))

ya_t2 <- ya %>%
  select(record_id,ya_hfs_date_t2,automode_start,ya_hfs_behave1_t2:ya_hfs_worry18_t2,ya_paid1_t2:ya_paid20_t2) %>%
  filter(!is.na(ya_hfs_date_t2),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(ya_hfs_date_t2,automode_start,units = "days"))
colnames(ya_t2) <- gsub("ya_|_t2","",colnames(ya_t2))

ya_long <- rbind(ya_baseline,ya_t1,ya_t2) %>%
  arrange(record_id,hfs_date)
ya_long$days_from_am <- as.numeric(ya_long$days_from_am)
ya_long$tpoint <- cut(ya_long$days_from_am,breaks = c(-Inf,0,45,135,225,Inf),
                         right = T,labels = c("B","M1","T1","T2","After T2"))
# Parents
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GParent_SurveyDATA_2019-04-26.csv")
parent <- read.csv(filename,na.strings = "",stringsAsFactors = F)

parent <- left_join(parent,am_start,by="record_id")
parent[,c("p_date","p_date_t1","p_date_t2","automode_start")] <- 
  lapply(parent[,c("p_date","p_date_t1","p_date_t2","automode_start")], lubridate::mdy)

parent_baseline <- parent %>%
  select(record_id,p_date,automode_start,p_hfs_behave1:p_hfs_worry18,p_paid1:p_paid18) %>%
  filter(!is.na(p_date),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(p_date,automode_start,units = "days"))
colnames(parent_baseline) <- sub("p_","",colnames(parent_baseline))

parent_t1 <- parent %>%
  select(record_id,p_date_t1,automode_start,p_hfs_behave1_t1:p_hfs_worry18_t1,p_paid1_t1:p_paid18_t1) %>%
  filter(!is.na(p_date_t1),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(p_date_t1,automode_start,units = "days"))
colnames(parent_t1) <- gsub("p_|_t1","",colnames(parent_t1))

parent_t2 <- parent %>%
  select(record_id,p_date_t2,automode_start,p_hfs_behave1_t2:p_hfs_worry18_t2,p_paid1_t2:p_paid18_t2) %>%
  filter(!is.na(p_date_t2),!is.na(automode_start)) %>%
  mutate(days_from_am = difftime(p_date_t2,automode_start,units = "days"))
colnames(parent_t2) <- gsub("p_|_t2","",colnames(parent_t2))

parent_long <- rbind(parent_baseline,parent_t1,parent_t2) %>%
  arrange(record_id,date)
parent_long$days_from_am <- as.numeric(parent_long$days_from_am)
parent_long$tpoint <- cut(parent_long$days_from_am,breaks = c(-Inf,0,45,135,225,Inf),
                         right = T,labels = c("B","M1","T1","T2","After T2"))
# Write CSVs
printvars <- c("record_id","date","automode_start","days_from_am","tpoint")
write.csv(child_long[,printvars],file = "/Users/timvigers/Desktop/child_survey_dates.csv",row.names = F,na = "")
write.csv(ya_long[,c("record_id","hfs_date","automode_start","days_from_am","tpoint")],
          file = "/Users/timvigers/Desktop/ya_survey_dates.csv",row.names = F,na = "")
write.csv(parent_long[,printvars],file = "/Users/timvigers/Desktop/parent_survey_dates.csv",row.names = F,na = "")
```

```{r echo=FALSE,eval=FALSE}
# YA survey scores
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GYoungAdult_SurveyDATA_2019-04-26_.csv")
ya <- read.csv(filename,na.strings = "",stringsAsFactors = F)
# Change column names
b_cols_paid <- paste0("ya_paid",seq(1:20),"_base")
t1_cols_paid <- paste0("ya_paid",seq(1:20),"_t1")
t2_cols_paid <- paste0("ya_paid",seq(1:20),"_t2")
b_cols_behavior <- paste0("ya_hfs_behave",1:15,"_b")
t1_cols_behavior <- paste0("ya_hfs_behave",1:15,"_t1")
# Note: manually changed column "ya_hfs_behave9_b_t1_t2" to "ya_hfs_behave9_t2"
t2_cols_behavior <- paste0("ya_hfs_behave",1:15,"_t2")
b_cols_worry <- paste0("ya_hfs_worry",1:18,"_b")
t1_cols_worry <- paste0("ya_hfs_worry",1:18,"_t1")
t2_cols_worry <- paste0("ya_hfs_worry",1:18,"_t2")
# Score by timepoint, gather into separate frame
ya$B <- apply(ya[,b_cols_paid],1,function(x) sum(x) * 1.25)
ya$T1 <- apply(ya[,t1_cols_paid],1,function(x) sum(x) * 1.25)
ya$T2 <- apply(ya[,t2_cols_paid],1,function(x) sum(x) * 1.25)
paid <- gather(ya[,c("record_id","B","T1","T2")],tpoint, paid_score, B:T2)

ya$B <- apply(ya[,b_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
ya$T1 <- apply(ya[,t1_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
ya$T2 <- apply(ya[,t2_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
behavior <- gather(ya[,c("record_id","B","T1","T2")],tpoint, behavior_score, B:T2)

ya$B <- apply(ya[,b_cols_worry],1,function(x) sum((x - 1),na.rm = T))
# Note: manually changed column "ya_hfs_worry8_b_t1" to "ya_hfs_worry8_t1"
ya$T1 <- apply(ya[,t1_cols_worry],1,function(x) sum((x - 1),na.rm = T))
ya$T2 <- apply(ya[,t2_cols_worry],1,function(x) sum((x - 1),na.rm = T))
worry <- gather(ya[,c("record_id","B","T1","T2")],tpoint, worry_score, B:T2)
# Gather, merge, and order all surveys. Calculate total HFS. Rename columns.
ya_surveys <- plyr::join_all(list(paid,behavior,worry), by = c("record_id","tpoint"))
ya_surveys <- ya_surveys[order(ya_surveys$record_id),]
ya_surveys$total_hfs <- ya_surveys$behavior_score + ya_surveys$worry_score
ya_surveys[ya_surveys == 0] <- NA
colnames(ya_surveys) <- c("record_id","tpoint","yapaid_score","yabehavior_score","yaworry_score","yatotal_score")
# Write CSV
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_ya_survey.csv")
write.csv(ya_surveys,file = filename,row.names = F,na = "")
```

```{r echo=FALSE,eval=FALSE}
# Parent survey scores
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GParent_SurveyDATA_2019-04-26.csv")
parent <- read.csv(filename,na.strings = "",stringsAsFactors = F)
# Column names for each survey
b_cols_paid <- paste0("p_paid",seq(1:18))
t1_cols_paid <- paste0(b_cols_paid,"_t1")
t2_cols_paid <- paste0(b_cols_paid,"_t2")
b_cols_behavior <- paste0("p_hfs_behave",c(1:11))
t1_cols_behavior <- paste0(b_cols_behavior,"_t1")
t2_cols_behavior <- paste0(b_cols_behavior,"_t2")
b_cols_worry <- paste0("p_hfs_worry",c(12:26))
t1_cols_worry <- paste0(b_cols_worry,"_t1")
t2_cols_worry <- paste0(b_cols_worry,"_t2")
# Score by timepoint, gather into separate frame
parent$B <- apply(parent[,b_cols_paid],1,function(x) mean((4 - (x - 1)),na.rm = T) * 25)
parent$T1 <- apply(parent[,t1_cols_paid],1,function(x) mean((4 - (x - 1)),na.rm = T) * 25)
parent$T2 <- apply(parent[,t2_cols_paid],1,function(x) mean((4 - (x - 1)),na.rm = T) * 25)
paid <- gather(parent[,c("record_id","B","T1","T2")],tpoint, paid_score, B:T2)

parent$B <- apply(parent[,b_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
parent$T1 <- apply(parent[,t1_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
parent$T2 <- apply(parent[,t2_cols_behavior],1,function(x) sum((x - 1),na.rm = T))
behavior <- gather(parent[,c("record_id","B","T1","T2")],tpoint, behavior_score, B:T2)

parent$B <- apply(parent[,b_cols_worry],1,function(x) sum((x - 1),na.rm = T))
parent$T1 <- apply(parent[,t1_cols_worry],1,function(x) sum((x - 1),na.rm = T))
parent$T2 <- apply(parent[,t2_cols_worry],1,function(x) sum((x - 1),na.rm = T))
worry <- gather(parent[,c("record_id","B","T1","T2")],tpoint, worry_score, B:T2)
# Gather, merge, and order all surveys, replace NaN with NA. Rename columns.
parent_surveys <- plyr::join_all(list(paid,behavior,worry), by = c("record_id","tpoint"))
parent_surveys <- parent_surveys[order(parent_surveys$record_id),]
parent_surveys$paid_score[is.nan(parent_surveys$paid_score)] <- NA
parent_surveys[parent_surveys == 0] <- NA
parent_surveys$total_hfs <- parent_surveys$behavior_score + parent_surveys$worry_score
colnames(parent_surveys) <- c("record_id","tpoint","ppaid_score","pbehavior_score","pworry_score","ptotal_score")
# Write CSV
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_parent_survey.csv")
write.csv(parent_surveys,file = filename,row.names = F,na = "")
```

# Methods

Linear mixed effects models were used to examine change in HbA1c over time. Three models were compared using Aikaikeâ€™s Information Criterion to select the best model:

1.	A random intercept model with visit number (baseline, T1, T2) as a categorical time variable.  No adjustment for baseline.
2.	A random intercept model with visit number as a continuous time variable, without adjustment for baseline.
3.	A random intercept and random slope model, without adjustment for baseline.

The random intercept model with time treated as a categorical variable was the best model. Originally the models were adjusted for baseline value (as we did for the abstract), but it was decided that this adjustment does not make sense given the question this study wants to answer, in addition to making the model results difficult to interpret.

The HbA1c model included an interaction term for baseline HbA1c group with timepoint, but the CGM variable models did not. We examined the interaction effect of age and timepoint on HbA1c, but it was not significant. Differences between timepoints for each baseline HbA1c group were compared using linear contrasts.

Discontinuation was defined as AM time < 10 at either T1 or T2. This analysis excluded those never trained on automode, and all timepoints with < 10 AM time.

# Results

```{r echo=FALSE,eval=FALSE}
# Import Cari's cleaned data
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_glycemic_data.csv")
alldata <- read.csv(filename,na.strings = "",stringsAsFactors = F,colClasses = c("record_id"="character"))
# Define variables of interest.
vars <- c("hba1c","am_time","mm_time","sensor_wear","sensor_u54","sensor_55_69",
          "sensor_70_180","sensor_181_250","sensor_g250","mean_sg","sd",
          "bg_checks","calibrations","tdd","basal","bolus","amexit",
          "amexit_day","amexit_hyper","amexit_hypo","amexit_manual","amexit_other")
# Import cleaned, scored survey data.
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_parent_survey.csv")
parent_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F)
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_child_survey.csv")
child_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F,colClasses = c("record_id"="character"))
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_ya_survey.csv")
ya_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F,colClasses = c("record_id"="character"))
# Add survey scores to full data.
alldata <- left_join(alldata,child_survey,by = c("record_id","tpoint"))
alldata <- left_join(alldata,ya_survey,by = c("record_id","tpoint"))
alldata <- full_join(alldata,parent_survey,by = c("record_id","tpoint"))
# Order, add baseline A1c
alldata <- alldata %>%
  arrange(record_id,tpoint) %>%
  group_by(record_id) %>%
  mutate(baseline_a1c = hba1c[1])
alldata$tpoint <- as.factor(alldata$tpoint)
alldata$record_id <- as.factor(alldata$record_id)
```

```{r echo=FALSE}
# Read in Cari's corrected data from 4/26/19
alldata <- read.csv(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/visits 670G obs_CBupdate_25APR2019.csv"),na.strings = "")
alldata <- alldata %>% arrange(record_id,tpoint)
# Never trained
no_am <- read.csv(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/no_am_start.csv"),na.strings = "")
never_trained <- no_am$never.started.AM[!is.na(no_am$never.started.AM)]
# Discontinuers
t1_discont <- unique(alldata$record_id[which(alldata$tpoint == "T1" & alldata$am_time < 10)])
# Remove participant 10 from discontinuers, because she had AM > 10 at T2.
t1_discont <- t1_discont[-c(which(t1_discont == "10"))]
t2_discont <- unique(alldata$record_id[which(alldata$tpoint == "T2" & alldata$am_time < 10)])
t2_discont <- t2_discont[!(t2_discont %in% t1_discont)]
# Remove those never trained from discontinuers
t1_discont <- t1_discont[-c(which(t1_discont %in% never_trained))]
t2_discont <- t2_discont[-c(which(t2_discont %in% never_trained))]
all_discont <- unique(c(t1_discont,t2_discont))
# If participant discontinued at T1, remove T1 and T2, if dicontinued at T2, remove just T2. Also remove those never trained.
alldata <- alldata %>% 
  filter(!(record_id %in% never_trained)) %>%
  filter(!(record_id %in% t1_discont & (tpoint == "T1" | tpoint == "T2"))) %>%
  filter(!(record_id %in% t2_discont & tpoint == "T2"))
```

```{r echo=FALSE,include=FALSE}
# Demographics
dem_vars <- c("hba1c_baseline","demographics_age","demographics_t1d_duration","demographics_ethnicity","demographics_race","demographics_sex","demographics_insurance","demographics_pumphx","demographics_cgmhx")
# Categorical variables
catvars <- c("demographics_ethnicity","demographics_race","demographics_sex","demographics_insurance","demographics_pumphx","demographics_cgmhx")
demographics <- read.csv(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_GlycemicDATA_2019-01-07_.csv"))
demographics[,catvars] <- lapply(demographics[,catvars],as.factor)
demographics <- demographics %>%
  select(record_id,dem_vars,hba1c_baseline)
demographics$age_group <- cut(demographics$demographics_age, 
                              breaks = c(0,14,18,Inf),
                              labels = c("< 14","14 - 17","18 +"),right = F)
dem_vars <- c(dem_vars,"age_group")
# Add days between visits
dates <- read.csv(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/670GChild_GlycemicDATA_2019-01-07_.csv"),na.strings = "") 
# Format
dates <- dates %>%
  select(record_id,hba1c_date_b,automode_start,t1_date_m1,t1_date,t2_date)
dates[,2:ncol(dates)] <- lapply(dates[,2:ncol(dates)],lubridate::mdy)
dates$automode_start[which(dates$record_id %in% never_trained)] <- NA
dates$baseline_a1c_to_am <- as.numeric(difftime(dates$automode_start,dates$hba1c_date_b),"days")
dates$am_to_m1 <- as.numeric(difftime(dates$t1_date_m1,dates$automode_start),"days")
dates$am_to_t1 <- as.numeric(difftime(dates$t1_date,dates$automode_start),"days")
dates$am_to_t2 <- as.numeric(difftime(dates$t2_date,dates$automode_start),"days")
# Select columns
dates <- dates %>%
  select(record_id,baseline_a1c_to_am,am_to_m1,am_to_t1,am_to_t2)
# Merge
demographics <- left_join(demographics,dates,by = "record_id")
# Group by continued, discontinued T1, and discontinued T2
demographics$continuation <- ifelse(demographics$record_id %in% t1_discont,"Discontinued T1",
                               ifelse(demographics$record_id %in% t2_discont,"Discontinued T2",
                                      ifelse(demographics$record_id %in% never_trained,"Never Trained","Continued")))
# Variables
dem_vars <- c("baseline_a1c_to_am","am_to_m1","am_to_t1","am_to_t2",dem_vars)
# Normality check
nonnormal <- norm.check(demographics,c("hba1c_baseline","baseline_a1c_to_am",
                                       "am_to_m1","am_to_t1","am_to_t2",
                                       "demographics_age","demographics_t1d_duration"))
# Continuers
t1 <- CreateTableOne(dem_vars,data = demographics,strata = "continuation")
t1 <- print(t1,nonnormal = nonnormal)
# Add age range
t1["demographics_age (mean (SD))","Continued"] <- 
  paste0(t1["demographics_age (mean (SD))","Continued"]," [",
         min(demographics$demographics_age[demographics$continuation == "Continued"],na.rm = T),", ",
         max(demographics$demographics_age[demographics$continuation == "Continued"],na.rm = T),"]")
t1["demographics_age (mean (SD))","Discontinued T1"] <- 
  paste0(t1["demographics_age (mean (SD))","Discontinued T1"]," [",
         min(demographics$demographics_age[demographics$continuation == "Discontinued T1"],na.rm = T),", ",
         max(demographics$demographics_age[demographics$continuation == "Discontinued T1"],na.rm = T),"]")
t1["demographics_age (mean (SD))","Discontinued T2"] <- 
  paste0(t1["demographics_age (mean (SD))","Discontinued T2"]," [",
         min(demographics$demographics_age[demographics$continuation == "Discontinued T2"],na.rm = T),", ",
         max(demographics$demographics_age[demographics$continuation == "Discontinued T2"],na.rm = T),"]")
t1["demographics_age (mean (SD))","Never Trained"] <- 
  paste0(t1["demographics_age (mean (SD))","Never Trained"]," [",
         min(demographics$demographics_age[demographics$continuation == "Never Trained"],na.rm = T),", ",
         max(demographics$demographics_age[demographics$continuation == "Never Trained"],na.rm = T),"]")
```

## Table 1a: Descriptive Statistics,
```{r echo=FALSE}
kable(t1)
```

## Figure 1: Mean HbA1c by Timepoint
```{r echo=FALSE,warning=FALSE,dpi=600}
# Add baseline age to all data.
alldata <- left_join(alldata,demographics[,c("record_id","age_group","hba1c_baseline")],by = "record_id")
# Remove M1, group by clinical A1c cutoffs at baseline
data_no_m1 <- alldata %>%
  filter(tpoint != "M1") %>%
  mutate(hba1c_clinical = cut(hba1c_baseline,breaks = c(0,7.5,9.0,Inf)))
# Write file for SAS contrasts
write.csv(data_no_m1,file = "/Users/timvigers/Desktop/cari.csv",
          row.names = F,na="")
# Remove B (e.g. for % time AM models etc.)
data_no_b <- alldata %>%
  filter(tpoint != "B",!(record_id %in% all_discont))
# A1c plot
alldata$record_id <- as.factor(alldata$record_id)
a1c_overall <- 
  ggplot(data_no_m1,aes_string(x = "tpoint",y = "hba1c")) + 
  geom_point(size = 0.2) +
  stat_summary(fun.y=mean, colour="red", geom="point") +
  stat_summary(fun.y=mean, colour="red", geom="line", aes(group = 1)) +
  xlab("Timepoint") + 
  ylab("HbA1c (%)") +
  theme(legend.title=element_blank())
a1c_overall
```

## Table 2: A1c Mixed Models by Baseline HbA1c Group

```{r echo=FALSE, include=FALSE}
# A1c mixed models
a1c_age_mod <- lme(hba1c ~ tpoint*age_group,random=~1|record_id,data = data_no_m1,na.action = na.omit)
a1c_age_mod_cont <- lme(hba1c ~ as.numeric(tpoint)*age_group,random=~1|record_id,data = data_no_m1,na.action = na.omit) # Categorical is better
a1c_glyc_control_mod <- lme(hba1c ~ tpoint*hba1c_clinical,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# Means models
a1c_age_mod_means <- lme(hba1c ~ tpoint:age_group-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
a1c_glyc_control_mod_means <- lme(hba1c ~ tpoint:hba1c_clinical-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# Contrasts
# cmat.emmc <- rbind(c(0, 0, 1, 0, 0, 0, 0, 0, -1),
#               c(1, 0, 0, 0, 0, 0, -1, 0, 0),
#               c(0, 1, 0, 0, 0, 0, 0, -1, 0),
#               c(0, 0, 0, 0, 0, 1, 0, 0, -1),
#               c(0, 0, 0, 1, 0, 0, -1, 0, 0),
#               c(0, 0, 0, 0, 1, 0, 0, -1, 0))
# a1c_glyc_emm <- emmeans(a1c_glyc_control_mod,~tpoint:hba1c_clinical)
# contrast(a1c_glyc_emm,"cmat")
```

```{r echo=FALSE}
# Import results from contrasts in SAS.
estimates <- read_excel(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Reports/estimates.xlsx"))
means <- read_excel(paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Reports/means.xlsx"))
```

```{r echo=FALSE}
kable(means[,c("tpoint","hba1c_clinical","Estimate","StdErr","Probt")],caption = "Group Means by Time Point")
kable(anova.lme(a1c_glyc_control_mod),caption = "Type 3 Tests of Fixed Effects")
kable(estimates[,c("Label","Estimate","StdErr","Probt")],caption = "Timepoint Differences by HbA1c Group")
```

### HbA1c mixed model interpretation
The "Group Means by Timepoint" table shows the average HbA1c at each timepoint, for each HbA1c group. The p values in this table ("Probt") refer to whether the mean HbA1c is close to 0, so they can be ignored for now. 

The type 3 tests of fixed effects show that timepoint, HbA1c group, and the interaction between timepoint and HbA1c group were each significant overall.

"Timepoint Differences by HbA1c Group" shows whether time 1 or time 2 were significantly different from baseline, by HbA1c group (Low = (0,7.5], Medium = (7.5,9], High = (9,Inf]). So on average, the low A1c group decreased by 0.03 from baseline to visit 1, but it was not statistically significant. On average the high A1c group decreased by 1.054 from baseline to visit 1, which was statistically significant (p < 0.0001).

```{r echo=FALSE}
# CGM data models
# TIR
tir_u54_mod <- lme(sensor_u54 ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tir_u54_mod_means <- lme(sensor_u54 ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)

tir_55_69_mod <- lme(sensor_55_69 ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tir_55_69_mod_means <- lme(sensor_55_69 ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)

tir_70_180_mod <- lme(sensor_70_180 ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tir_70_180_mod_means <- lme(sensor_70_180 ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)

tir_181_250_mod <- lme(sensor_181_250 ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tir_181_250_mod_means <- lme(sensor_181_250 ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)

tir_g250_mod <- lme(sensor_g250 ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tir_g250_mod_means <- lme(sensor_g250 ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# AM time
am_mod <- lme(am_time ~ tpoint,random=~1|record_id,data = data_no_b,na.action = na.omit)
am_mod_means <- lme(am_time ~ tpoint-1,random=~1|record_id,data = data_no_b,na.action = na.omit)
# Sensor wear
sensor_wear_mod <- lme(sensor_wear ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
sensor_wear_mod_means <- lme(sensor_wear ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# AM exits
amexit_day_mod <- lme(amexit_day ~ tpoint,random=~1|record_id,data = data_no_b,na.action = na.omit)
amexit_day_mod_means <- lme(amexit_day ~ tpoint-1,random=~1|record_id,data = data_no_b,na.action = na.omit)
# BG Checks
bg_checks_mod <- lme(bg_checks ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
bg_checks_mod_means <- lme(bg_checks ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# Calibrations
calibrations_mod <- lme(calibrations ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
calibrations_mod_means <- lme(calibrations ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# TDD
tdd_mod <- lme(tdd ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
tdd_mod_means <- lme(tdd ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# Basal
basal_mod <- lme(basal ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
basal_mod_means <- lme(basal ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
# Bolus
bolus_mod <- lme(bolus ~ tpoint,random=~1|record_id,data = data_no_m1,na.action = na.omit)
bolus_mod_means <- lme(bolus ~ tpoint-1,random=~1|record_id,data = data_no_m1,na.action = na.omit)
```

## Table 3: CGM Variable Mixed Models Summary

```{r echo=FALSE}
# Make table
mod_summary <- as.data.frame(matrix(nrow = 13,ncol = 6))
colnames(mod_summary) <- c("Baseline","Month 1","Time 1","T1 P value","Time 2","T2 P value")
rownames(mod_summary) <- c("TIR Under 54","TIR 55-69","TIR 70-180",
                           "TIR 181-250","TIR >250","AM Time","Sensor Wear",
                           "AM Exits Per Day","BG Checks","Calibrations","TDD",
                           "Basal","Bolus")
# TIR
tir_u54 <- paste0(round(summary(tir_u54_mod_means)$tTable[,1],3)," (",
              round(summary(tir_u54_mod_means)$tTable[,2],3),")")
mod_summary["TIR Under 54","Baseline"] <- tir_u54[1]
mod_summary["TIR Under 54","Time 1"] <- tir_u54[2]
mod_summary["TIR Under 54","T1 P value"] <- summary(tir_u54_mod)$tTable[2,5]
mod_summary["TIR Under 54","Time 2"] <- tir_u54[3]
mod_summary["TIR Under 54","T2 P value"] <- summary(tir_u54_mod)$tTable[3,5]

tir_55_69 <- paste0(round(summary(tir_55_69_mod_means)$tTable[,1],3)," (",
              round(summary(tir_55_69_mod_means)$tTable[,2],3),")")
mod_summary["TIR 55-69","Baseline"] <- tir_55_69[1]
mod_summary["TIR 55-69","Time 1"] <- tir_55_69[2]
mod_summary["TIR 55-69","T1 P value"] <- summary(tir_55_69_mod)$tTable[2,5]
mod_summary["TIR 55-69","Time 2"] <- tir_55_69[3]
mod_summary["TIR 55-69","T2 P value"] <- summary(tir_55_69_mod)$tTable[3,5]

tir_70_180 <- paste0(round(summary(tir_70_180_mod_means)$tTable[,1],3)," (",
              round(summary(tir_70_180_mod_means)$tTable[,2],3),")")
mod_summary["TIR 70-180","Baseline"] <- tir_70_180[1]
mod_summary["TIR 70-180","Time 1"] <- tir_70_180[2]
mod_summary["TIR 70-180","T1 P value"] <- summary(tir_70_180_mod)$tTable[2,5]
mod_summary["TIR 70-180","Time 2"] <- tir_70_180[3]
mod_summary["TIR 70-180","T2 P value"] <- summary(tir_70_180_mod)$tTable[3,5]

tir_181_250 <- paste0(round(summary(tir_181_250_mod_means)$tTable[,1],3)," (",
              round(summary(tir_181_250_mod_means)$tTable[,2],3),")")
mod_summary["TIR 181-250","Baseline"] <- tir_181_250[1]
mod_summary["TIR 181-250","Time 1"] <- tir_181_250[2]
mod_summary["TIR 181-250","T1 P value"] <- summary(tir_181_250_mod)$tTable[2,5]
mod_summary["TIR 181-250","Time 2"] <- tir_181_250[3]
mod_summary["TIR 181-250","T2 P value"] <- summary(tir_181_250_mod)$tTable[3,5]

tir_g250 <- paste0(round(summary(tir_g250_mod_means)$tTable[,1],3)," (",
              round(summary(tir_g250_mod_means)$tTable[,2],3),")")
mod_summary["TIR >250","Baseline"] <- tir_g250[1]
mod_summary["TIR >250","Time 1"] <- tir_g250[2]
mod_summary["TIR >250","T1 P value"] <- summary(tir_g250_mod)$tTable[2,5]
mod_summary["TIR >250","Time 2"] <- tir_g250[3]
mod_summary["TIR >250","T2 P value"] <- summary(tir_g250_mod)$tTable[3,5]

# AM time
am <- paste0(round(summary(am_mod_means)$tTable[,1],3)," (",
             round(summary(am_mod_means)$tTable[,2],3),")")
mod_summary["AM Time","Month 1"] <- am[1]
mod_summary["AM Time","Time 1"] <- am[2]
mod_summary["AM Time","T1 P value"] <- summary(am_mod)$tTable[2,5]
mod_summary["AM Time","Time 2"] <- am[3]
mod_summary["AM Time","T2 P value"] <- summary(am_mod)$tTable[3,5]

# Sensor
sensor_wear <- paste0(round(summary(sensor_wear_mod_means)$tTable[,1],3)," (",
                      round(summary(sensor_wear_mod_means)$tTable[,2],3),")")
mod_summary["Sensor Wear","Baseline"] <- sensor_wear[1]
mod_summary["Sensor Wear","Time 1"] <- sensor_wear[2]
mod_summary["Sensor Wear","T1 P value"] <- summary(sensor_wear_mod)$tTable[2,5]
mod_summary["Sensor Wear","Time 2"] <- sensor_wear[3]
mod_summary["Sensor Wear","T2 P value"] <- summary(sensor_wear_mod)$tTable[3,5]

# AM Exits Per Day
amexit_day <- paste0(round(summary(amexit_day_mod_means)$tTable[,1],3)," (",
                     round(summary(amexit_day_mod_means)$tTable[,2],3),")")
mod_summary["AM Exits Per Day","Month 1"] <- amexit_day[1]
mod_summary["AM Exits Per Day","Time 1"] <- amexit_day[2]
mod_summary["AM Exits Per Day","T1 P value"] <- summary(amexit_day_mod)$tTable[2,5]
mod_summary["AM Exits Per Day","Time 2"] <- amexit_day[3]
mod_summary["AM Exits Per Day","T2 P value"] <- summary(amexit_day_mod)$tTable[3,5]

# BG Checks
bg_checks <- paste0(round(summary(bg_checks_mod_means)$tTable[,1],3)," (",
              round(summary(bg_checks_mod_means)$tTable[,2],3),")")
mod_summary["BG Checks","Baseline"] <- bg_checks[1]
mod_summary["BG Checks","Time 1"] <- bg_checks[2]
mod_summary["BG Checks","T1 P value"] <- summary(bg_checks_mod)$tTable[2,5]
mod_summary["BG Checks","Time 2"] <- bg_checks[3]
mod_summary["BG Checks","T2 P value"] <- summary(bg_checks_mod)$tTable[3,5]
# Calibrations
calibrations <- paste0(round(summary(calibrations_mod_means)$tTable[,1],3)," (",
              round(summary(calibrations_mod_means)$tTable[,2],3),")")
mod_summary["Calibrations","Baseline"] <- calibrations[1]
mod_summary["Calibrations","Time 1"] <- calibrations[2]
mod_summary["Calibrations","T1 P value"] <- summary(calibrations_mod)$tTable[2,5]
mod_summary["Calibrations","Time 2"] <- calibrations[3]
mod_summary["Calibrations","T2 P value"] <- summary(calibrations_mod)$tTable[3,5]
# TDD
tdd <- paste0(round(summary(tdd_mod_means)$tTable[,1],3)," (",
              round(summary(tdd_mod_means)$tTable[,2],3),")")
mod_summary["TDD","Baseline"] <- tdd[1]
mod_summary["TDD","Time 1"] <- tdd[2]
mod_summary["TDD","T1 P value"] <- summary(tdd_mod)$tTable[2,5]
mod_summary["TDD","Time 2"] <- tdd[3]
mod_summary["TDD","T2 P value"] <- summary(tdd_mod)$tTable[3,5]
# Basal
basal <- paste0(round(summary(basal_mod_means)$tTable[,1],3)," (",
              round(summary(basal_mod_means)$tTable[,2],3),")")
mod_summary["Basal","Baseline"] <- basal[1]
mod_summary["Basal","Time 1"] <- basal[2]
mod_summary["Basal","T1 P value"] <- summary(basal_mod)$tTable[2,5]
mod_summary["Basal","Time 2"] <- basal[3]
mod_summary["Basal","T2 P value"] <- summary(basal_mod)$tTable[3,5]
# Bolus
bolus <- paste0(round(summary(bolus_mod_means)$tTable[,1],3)," (",
              round(summary(bolus_mod_means)$tTable[,2],3),")")
mod_summary["Bolus","Baseline"] <- bolus[1]
mod_summary["Bolus","Time 1"] <- bolus[2]
mod_summary["Bolus","T1 P value"] <- summary(bolus_mod)$tTable[2,5]
mod_summary["Bolus","Time 2"] <- bolus[3]
mod_summary["Bolus","T2 P value"] <- summary(bolus_mod)$tTable[3,5]
# Adjust p values, format
mod_summary$`T1 P value` <- format.pval(p.adjust(mod_summary$`T1 P value`,"fdr"),
                                        eps = 0.001,digits = 2)
mod_summary$`T2 P value` <- format.pval(p.adjust(mod_summary$`T2 P value`,"fdr"),
                                        eps = 0.001,digits = 2)
# Print
options(knitr.kable.NA = '')
kable(mod_summary)
```

Reported as mean (standard error). All continuers, no interaction by HbA1c group included in these models. P values adjusted for multiple comparisons using the false discovery rate (FDR) method. 

## Figure 2: Survey Scores by Timepoint

```{r echo=FALSE,warning=FALSE,dpi=600,eval=FALSE}
# Import cleaned, scored survey data.
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_parent_survey.csv")
parent_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F)
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_child_survey.csv")
child_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F,colClasses = c("record_id"="character"))
filename <- paste0(pathstart,"som/PEDS/RI Biostatistics Core/Shared/Shared Projects/Laura/BDC/Projects/Cari Berget/6 Month Manuscript/Data_Cleaned/clean_ya_survey.csv")
ya_survey <- read.csv(filename,na.strings = "",stringsAsFactors = F,colClasses = c("record_id"="character"))
# Remove incorrect scores

# Make a new dataframe for PAID
paid_plot_data <- data_no_m1 %>%
  select(record_id,tpoint,cpaid_score,yapaid_score)
paid_plot_data$score <- pmax(paid_plot_data$cpaid_score,paid_plot_data$yapaid_score,na.rm = T)
paid_plot_data$group <- ifelse(!is.na(paid_plot_data$cpaid_score),"Kids",ifelse(!is.na(paid_plot_data$yapaid_score),"YA",NA))
paid_plot_data <- paid_plot_data[,c("record_id","tpoint","score","group")]
# Parents
parent_paid <- data_no_m1 %>%
  select(record_id,tpoint,ppaid_score)
parent_paid$record_id <- paste0(parent_paid$record_id,"p")
colnames(parent_paid) <- c("record_id","tpoint","score")
parent_paid$group <- "Parents"
# Bind
paid_plot_data <- rbind(paid_plot_data,parent_paid)
# PAID plot
paid_plot <-
  ggplot(paid_plot_data,aes(x = tpoint,y = score,group = record_id)) +
  geom_point(size = 0.2,aes(color = group)) +
  stat_summary(fun.y=mean, geom="line", aes(group = group,color = group)) +
  xlab("Timepoint") +
  ylab("PAID Score") +
  theme(legend.title=element_blank()) +
  scale_color_discrete(limits = c("Parents","YA","Kids"))
# FOH plots
# Make a new dataframes for each age group
foh_p_data <- data_no_m1 %>%
  select(record_id,tpoint,pworry_score)
foh_p_data <- melt(foh_p_data,id.vars = c("record_id","tpoint"))

foh_c_data <- data_no_m1 %>%
  select(record_id,tpoint,cworry_score)
foh_c_data <- melt(foh_c_data,id.vars = c("record_id","tpoint"))

foh_ya_data <- data_no_m1 %>%
  select(record_id,tpoint,yaworry_score)
foh_ya_data <- melt(foh_ya_data,id.vars = c("record_id","tpoint"))
# Points and means
foh_p_plot <- ggplot(foh_p_data,aes(x=tpoint,y=value)) +
  geom_point(size = 0.2) +
  stat_summary(fun.y=mean, geom="line", aes(group = variable)) +
  ggtitle("Parents") +
  xlab("Timepoint") +
  ylab("FOH Worry")
# Kids
foh_c_plot <- ggplot(foh_c_data,aes(x=tpoint,y=value)) +
  geom_point(size = 0.2) +
  stat_summary(fun.y=mean, geom="line", aes(group = variable)) +
  ggtitle("Kids") +
  xlab("Timepoint") +
  ylab("FOH Worry")
# YA
foh_ya_plot <- ggplot(foh_ya_data,aes(x=tpoint,y=value)) +
  geom_point(size = 0.2) +
  stat_summary(fun.y=mean, geom="line", aes(group = variable)) +
  ggtitle("YA") +
  xlab("Timepoint") +
  ylab("FOH Worry")
# Panels
grid.arrange(paid_plot,foh_p_plot,foh_c_plot,foh_ya_plot,nrow = 2)
```

```{r eval=FALSE,include=FALSE}
# General notes
# List of manual data changes
# Renames columns:
# sensor_54_69_m1 to sensor_55_69_m1
# baseline data sensor_54_69 to sensor_55_69
# all "amexits" to "amexit"

# Additional data notes
# Using a strict 3 month +/- 45 days window would have resulted in the loss of quite a bit of data, so visit dates were manually re-classified by Cari Berget. 
# The cleaned data was sent to Cari Berget due to several concerning outliers. She corrected the data and sent it back, so this chunk of code no longer needs to run. The cleaned data from Cari is just imported instead.  

# During 4/12/19 meeting we decided that adjusting the models for baseline value
# doesn't really answer the scientific question. Also Cari would like to compare
# baseline to 3 month and 6 month. Month 1 A1c overlaps too much with baseline. 
# Briggs also requested new figures grouped by glycemic control (7.5, 9.0, and 
# above, plus tertiles/quartiles).

# Tried to plot three FOH subscales for parents and kids together, but the plot 
# was really messy, so split the plots by age group.

# Exclude extra parents (1a and 74a)

# Per meeting with Cari 4/26/19: 
# 32's T1 survey scores are incorrect, exclude them - done in data CSV for now. 1a and 74a also deleted manually from CSV file.
# Check survey timepoints to make sure the first survey taken was T1 (e.g. some people don't follow up for a long time and survey 1 might be closer to T4)
# Some survey scores are actually 0, not missing so make sure they're treated that way. Maybe if their other scores are nonzero at that timepoint, don't convert 0  to NA?
# Split discontinuers into at T1 and at T2 - done
# Cari found a few other participants who were never trained on AM, add to code - done
```